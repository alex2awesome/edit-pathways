{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sqlite3\n",
    "import sys\n",
    "sys.path.insert(0, '../util')\n",
    "import util_refactorings as ur\n",
    "from importlib import reload\n",
    "reload(ur)\n",
    "import glob\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences_add(doc):\n",
    "    doc = doc.copy()\n",
    "    doc = doc.loc[lambda df:~df[['sent_idx_x', 'sent_idx_y']].isnull().all(axis=1)]\n",
    "    sent_idxs = doc['sent_idx_y'].dropna().sort_values().tolist()\n",
    "    additions = doc.loc[lambda df: df['sent_idx_x'].isnull()]['sent_idx_y'].tolist()\n",
    "    \n",
    "    add_aboves = []\n",
    "    add_belows = []\n",
    "    idx_in_add_l = 0\n",
    "    while idx_in_add_l < len(additions):\n",
    "        a = additions[idx_in_add_l]\n",
    "        idx_in_sent_l = sent_idxs.index(a)\n",
    "        cluster_size = 1\n",
    "        if idx_in_sent_l < len(sent_idxs) - cluster_size:\n",
    "            add_above = sent_idxs[idx_in_sent_l + cluster_size]\n",
    "            exists_sent_below = True\n",
    "            while add_above in additions:\n",
    "                cluster_size += 1\n",
    "                if (idx_in_sent_l + cluster_size) < len(sent_idxs):\n",
    "                    add_above = sent_idxs[idx_in_sent_l + cluster_size]\n",
    "                    exists_sent_below = True\n",
    "                else:\n",
    "                    exists_sent_below = False\n",
    "                    break\n",
    "            if exists_sent_below:\n",
    "                add_aboves.append({\n",
    "                    'add_above': add_above,\n",
    "                    'cluster_size': cluster_size\n",
    "                })\n",
    "        if idx_in_sent_l > 0:\n",
    "            add_below = sent_idxs[idx_in_sent_l - 1]\n",
    "            add_belows.append({\n",
    "                'add_below': add_below,\n",
    "                'cluster_size': cluster_size\n",
    "            })\n",
    "        idx_in_add_l += cluster_size\n",
    "    \n",
    "    return add_aboves, add_belows\n",
    "\n",
    "## label each sentence in the old version as:\n",
    "# 1. Deleted in the new version\n",
    "# 2. Sentence added above/sentence added below  \n",
    "# 3. Sentence edited\n",
    "# 4. Sentence refactored\n",
    "\n",
    "# 5. Sentence split (?)\n",
    "# 6. Sentence merge (?)\n",
    "\n",
    "def get_sentence_and_doc_labels(doc, doc_sentences):\n",
    "    # 1. Detect deletions:\n",
    "    deleted_labeled_sentences = pd.concat([\n",
    "        (doc_sentences\n",
    "         .loc[lambda df: ~df['sent_idx'].isin(doc['sent_idx_x'].dropna())]\n",
    "         .assign(deleted_label=True)\n",
    "         .rename(columns={'version':'version_x', 'sent_idx':'sent_idx_x'})\n",
    "         [['entry_id', 'version_x', 'sent_idx_x', 'deleted_label']]\n",
    "        )\n",
    "        ,\n",
    "        (doc\n",
    "         .loc[lambda df: df['sent_idx_y'].isnull()]\n",
    "          .assign(deleted_label=True)\n",
    "          [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'deleted_label']]\n",
    "        )\n",
    "    ]).drop_duplicates()\n",
    "\n",
    "    # 2. Sentence additions above/below\n",
    "    add_aboves, add_belows = label_sentences_add(doc)\n",
    "    if len(add_aboves) > 0:\n",
    "        add_above_labeled_sentences = (pd.DataFrame(add_aboves)\n",
    "        #  .assign(add_above_label=lambda df: df['cluster_size'].apply(lambda x: 'add above ' + str(x)))\n",
    "         .rename(columns={'cluster_size': 'add_above_label'})\n",
    "         .merge(doc, how='inner', right_on='sent_idx_y', left_on='add_above')\n",
    "         [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'add_above_label']]\n",
    "        )\n",
    "    else:\n",
    "        add_above_labeled_sentences = pd.DataFrame()\n",
    "        \n",
    "    if len(add_belows) > 0:\n",
    "        add_below_labeled_sentences = (pd.DataFrame(add_belows)\n",
    "        #  .assign(add_below_label=lambda df: df['cluster_size'].apply(lambda x: 'add below ' + str(x))) \n",
    "         .rename(columns={'cluster_size': 'add_below_label'})\n",
    "         .merge(doc, how='inner', right_on='sent_idx_y', left_on='add_below')\n",
    "         [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'add_below_label']]\n",
    "        )\n",
    "    else:\n",
    "        add_below_labeled_sentences = pd.DataFrame()\n",
    "#         doc['add_below_label'] = 0 \n",
    "\n",
    "    # 3. Sentence edits:\n",
    "    edited_sentences = (doc\n",
    "     .loc[lambda df: df['sent_idx_y'].notnull() & df['sent_idx_x'].notnull() & (df['avg_sentence_distance_x'] > .01)]\n",
    "     .assign(edited_label=True)\n",
    "      [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'edited_label']]\n",
    "    )\n",
    "    unchanged_sentences = (doc\n",
    "     .loc[lambda df: df['sent_idx_y'].notnull() & df['sent_idx_x'].notnull() & (df['avg_sentence_distance_x'] <= .01)]\n",
    "     .assign(unchanged_label=True)\n",
    "      [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'unchanged_label']]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 4. Sentence Refactors\n",
    "    refactors = ur.find_refactors_for_doc(doc)\n",
    "    refactored_sentences = (doc\n",
    "     .loc[lambda df: df.apply(lambda x: (x['sent_idx_x'], x['sent_idx_y']) in refactors, axis=1)]\n",
    "     .assign(refactored_label=lambda df: \n",
    "             df\n",
    "             .pipe(lambda df: df['sent_idx_y'] - df['sent_idx_x'])\n",
    "    #          .apply(lambda x: 'move %(direction)s %(num_steps)s' % ({\n",
    "    #              'direction': 'up' if x < 0 else 'down',\n",
    "    #              'num_steps': abs(int(x))\n",
    "    #              }))\n",
    "            )\n",
    "       [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'refactored_label']]\n",
    "    )\n",
    "\n",
    "    ## Concat to make Sentence-Level DF \n",
    "    sentence_label_df = (pd.concat([\n",
    "        deleted_labeled_sentences,\n",
    "        add_above_labeled_sentences,\n",
    "        add_below_labeled_sentences,\n",
    "        edited_sentences,\n",
    "        unchanged_sentences,\n",
    "        refactored_sentences,\n",
    "    ])\n",
    "     .assign(version_y=lambda df: df['version_y'].fillna(method='bfill'))\n",
    "     .fillna(False)\n",
    "     .astype(int)\n",
    "    )\n",
    "    if 'add_below_label' not in sentence_label_df:\n",
    "        sentence_label_df['add_below_label'] = 0\n",
    "    if 'add_above_label' not in sentence_label_df:\n",
    "        sentence_label_df['add_above_label'] = 0\n",
    "    \n",
    "    sentence_label_df = (sentence_label_df\n",
    "         .groupby(['entry_id', 'version_x', 'sent_idx_x'])\n",
    "         .agg({\n",
    "             'deleted_label': lambda s: max(s),\n",
    "             'add_above_label': lambda s: max(s),\n",
    "             'add_below_label': lambda s: max(s),\n",
    "             'edited_label': lambda s: max(s),\n",
    "             'unchanged_label': lambda s: max(s),\n",
    "             'refactored_label': lambda s: min(s)\n",
    "         })\n",
    "         .reset_index()\n",
    "        )\n",
    "    \n",
    "    sentence_label_df = doc_sentences.merge(\n",
    "            sentence_label_df,\n",
    "            how='left',\n",
    "            left_on=['entry_id', 'version', 'sent_idx'],\n",
    "            right_on=['entry_id', 'version_x', 'sent_idx_x']\n",
    "        ).drop(['version_x', 'sent_idx_x'], axis=1).fillna(0)\n",
    "        \n",
    "    ## Make Doc-Label DF\n",
    "    doc_label_df = (sentence_label_df\n",
    "     .assign(refactored_label=lambda df: (df['refactored_label'] != 0).astype(int))\n",
    "     .groupby(['entry_id', 'version'])\n",
    "     .aggregate({\n",
    "         'deleted_label':'sum',\n",
    "         'add_above_label':'sum',\n",
    "         'edited_label': 'sum',\n",
    "         'refactored_label': 'sum',\n",
    "         'sentence': lambda s: '<SENT>'.join(s)\n",
    "     })\n",
    "     .rename(columns={\n",
    "         'deleted_label': 'num_deleted',\n",
    "         'add_above_label': 'num_added',\n",
    "         'edited_label': 'num_edited',\n",
    "         'refactored_label': 'num_refactored',\n",
    "         'sentence': 'sentences'\n",
    "     })\n",
    "    )   \n",
    "    \n",
    "    return sentence_label_df, doc_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_and_matched_dfs(conn, sents_max=30, sents_min=3):\n",
    "    low_count_versions = pd.read_sql('''\n",
    "    with c1 as \n",
    "        (SELECT entry_id, \n",
    "            CAST(version as INT) as version, \n",
    "            COUNT(1) as c from split_sentences \n",
    "            GROUP BY entry_id, version)\n",
    "    SELECT entry_id, version from c1\n",
    "    WHERE c < %s and c > %s\n",
    "    '''% (sents_max, sents_min), con=conn)\n",
    "\n",
    "    # get join keys\n",
    "    low_count_entry_ids = ', '.join(list(map(str, low_count_versions['entry_id'].unique())))\n",
    "    joint_keys = low_count_versions.pipe(lambda df: df['entry_id'].astype(str) + '-' + df['version'].astype(str))\n",
    "    joint_keys = \"'%s'\" % \"', '\".join(joint_keys.tolist())\n",
    "\n",
    "    # matched sentences\n",
    "    matched_sentences = pd.read_sql('''\n",
    "        WITH c1 as ( \n",
    "        SELECT *, \n",
    "        entry_id || '-' || version_x as key_x,\n",
    "        entry_id || '-' || version_y as key_y \n",
    "        FROM matched_sentences \n",
    "        )\n",
    "        SELECT *\n",
    "        FROM c1\n",
    "        WHERE key_x in (%s) AND key_y  in (%s)\n",
    "        ''' % (joint_keys, joint_keys)\n",
    "    , con=conn)\n",
    "\n",
    "    # get split sentences\n",
    "    split_sentences = pd.read_sql('''\n",
    "        with c1 AS (\n",
    "            SELECT *, entry_id || '-' || CAST(version AS INT) as key FROM split_sentences\n",
    "        )\n",
    "        SELECT entry_id, CAST(version AS INT) as version, sent_idx, sentence \n",
    "        FROM c1\n",
    "        WHERE key IN (%s)\n",
    "    ''' % joint_keys, con=conn)\n",
    "    return matched_sentences, split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/diffengine-diffs/spark-output/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../modeling/data/doc*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../modeling/data/doc-data-independent.csv\n",
      "../modeling/data/doc-data-wp.csv\n",
      "../modeling/data/doc-data-reuters.csv\n",
      "../modeling/data/doc-data-small.csv\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    with open(f) as fr:\n",
    "        header= fr.readlines()[:1]\n",
    "        if 'entry_id' not in header[0]:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entry_id,version,num_deleted,add_above_label,num_added,num_edited,num_unchanged,num_refactored,sentences\\n']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_do = ['independent',\n",
    "'wp',\n",
    "'reuters',]\n",
    "files = list(filter(lambda x: any(list(map(lambda y: y in x, to_do))), files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching data for independent ...\n",
      "calculating statistics...\n",
      "processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d592f56a5c134992881f1165806f0cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to disk...\n",
      "fetching data for reuters ...\n",
      "calculating statistics...\n",
      "processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df25f84d6af48a6a676fda56e10fa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to disk...\n",
      "fetching data for guardian ...\n",
      "calculating statistics...\n",
      "processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640e9b7019ab411baa3dbd29d94bec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "nan is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-23ac445232c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mall_matched_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mall_split_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msentence_label_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_label_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_and_doc_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mall_sentence_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_label_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mall_doc_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_label_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b51c8a55caeb>\u001b[0m in \u001b[0;36mget_sentence_and_doc_labels\u001b[0;34m(doc, doc_sentences)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# 2. Sentence additions above/below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0madd_aboves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_belows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_sentences_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_aboves\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         add_above_labeled_sentences = (pd.DataFrame(add_aboves)\n",
      "\u001b[0;32m<ipython-input-2-b51c8a55caeb>\u001b[0m in \u001b[0;36mlabel_sentences_add\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0midx_in_add_l\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madditions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madditions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_add_l\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0midx_in_sent_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mcluster_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx_in_sent_l\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_idxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcluster_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: nan is not in list"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    source = f.split('/')[-1].split('-')[0]\n",
    "    conn = sqlite3.connect(f)\n",
    "    print('fetching data for %s ...' % source)\n",
    "    matched_sentences, split_sentences = get_split_and_matched_dfs(conn)\n",
    "\n",
    "    print('calculating statistics...')\n",
    "    edit_statistics = (matched_sentences\n",
    "     .groupby(['entry_id', 'version_x', 'version_y'])\n",
    "     .apply(lambda df: pd.Series({\n",
    "         'mean x dist': df['avg_sentence_distance_x'].mean(),\n",
    "         'mean y dist': df['avg_sentence_distance_y'].mean(),\n",
    "         'num_deleted' : df['sent_idx_y'].isnull().sum(),\n",
    "         'num_added' : df['sent_idx_x'].isnull().sum(),\n",
    "         'refactors': ur.find_refactors_for_doc(\n",
    "             df[['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'sent_idx_y']].dropna().astype(int)\n",
    "         ),\n",
    "     }))\n",
    "     .assign(num_refactors=lambda df: df['refactors'].str.len())\n",
    "     .assign(overall_mean=lambda df: df[['mean x dist', 'mean y dist']].mean(axis=1))\n",
    "    )\n",
    "\n",
    "    desired_docs = (edit_statistics\n",
    "     .loc[lambda df:\n",
    "          (df['overall_mean'] > .01) |\n",
    "          (df['num_deleted'] > 0) |\n",
    "          (df['num_added'] > 0) |\n",
    "          (df['num_refactors'] > 0)]\n",
    "    )\n",
    "\n",
    "\n",
    "    print('processing documents...')\n",
    "    all_sentence_labels = []\n",
    "    all_doc_labels = []\n",
    "    # for sanity checking\n",
    "    all_matched_sentences = []\n",
    "    all_split_sentences = []\n",
    "\n",
    "    for entry_id, v_x, v_y in tqdm(desired_docs.index):\n",
    "        doc = (matched_sentences\n",
    "         .loc[lambda df: (df['entry_id'] == entry_id) & (df['version_x'] == v_x) & (df['version_y'] == v_y)]\n",
    "         .sort_values(['sent_idx_x', 'sent_idx_y'])\n",
    "        )\n",
    "\n",
    "        doc_sentences = (split_sentences\n",
    "         .loc[lambda df: (df['entry_id'] == entry_id) & (df['version'] == v_x) ]\n",
    "                         .sort_values('sent_idx')\n",
    "        )\n",
    "\n",
    "        all_matched_sentences.append(doc)\n",
    "        all_split_sentences.append(doc_sentences)\n",
    "        sentence_label_df, doc_label_df = get_sentence_and_doc_labels(doc, doc_sentences)\n",
    "        all_sentence_labels.append(sentence_label_df)\n",
    "        all_doc_labels.append(doc_label_df)\n",
    "\n",
    "    print('writing to disk...')\n",
    "    all_doc_labels_df = pd.concat(all_doc_labels)\n",
    "    all_doc_labels_df.to_csv('../modeling/data/doc-data-%s.csv' % source, index=False)\n",
    "    all_sentence_labels_df = pd.concat(all_sentence_labels)\n",
    "    ## check \n",
    "    assert (all_sentence_labels_df[['edited_label', 'unchanged_label', 'deleted_label']]\n",
    "            .sum(axis=1)\n",
    "            .pipe(lambda s: s == 1)\n",
    "            .all()\n",
    "           )\n",
    "    all_sentence_labels_df.to_csv('../modeling/data/sentence-data-%s.csv' % source, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6892dfe9d17f4054a6c6cc416e299ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to disk...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6b29f36e1c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m assert (all_sentence_labels_df[['edited_label', 'unchanged_label', 'deleted_label']]\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m        )\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for entry_id, v_x, v_y in tqdm(desired_docs.index[61071:]):\n",
    "    doc = (matched_sentences\n",
    "     .loc[lambda df: (df['entry_id'] == entry_id) & (df['version_x'] == v_x) & (df['version_y'] == v_y)]\n",
    "     .sort_values(['sent_idx_x', 'sent_idx_y'])\n",
    "    )\n",
    "\n",
    "    doc_sentences = (split_sentences\n",
    "     .loc[lambda df: (df['entry_id'] == entry_id) & (df['version'] == v_x) ]\n",
    "                     .sort_values('sent_idx')\n",
    "    )\n",
    "\n",
    "    all_matched_sentences.append(doc)\n",
    "    all_split_sentences.append(doc_sentences)\n",
    "    sentence_label_df, doc_label_df = get_sentence_and_doc_labels(doc, doc_sentences)\n",
    "    all_sentence_labels.append(sentence_label_df)\n",
    "    all_doc_labels.append(doc_label_df)\n",
    "\n",
    "print('writing to disk...')\n",
    "all_doc_labels_df = pd.concat(all_doc_labels)\n",
    "all_doc_labels_df.to_csv('../modeling/data/doc-data-%s.csv' % source, index=False)\n",
    "all_sentence_labels_df = pd.concat(all_sentence_labels)\n",
    "## check \n",
    "assert (all_sentence_labels_df[['edited_label', 'unchanged_label', 'deleted_label']]\n",
    "        .sum(axis=1)\n",
    "        .pipe(lambda s: s == 1)\n",
    "        .all()\n",
    "       )\n",
    "all_sentence_labels_df.to_csv('../modeling/data/sentence-data-%s.csv' % source, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>version</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>deleted_label</th>\n",
       "      <th>add_above_label</th>\n",
       "      <th>add_below_label</th>\n",
       "      <th>edited_label</th>\n",
       "      <th>unchanged_label</th>\n",
       "      <th>refactored_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>823187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>South Korea’s much-loved K-pop star Lee Seung-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>823187</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>South Korea’s much-loved K-pop star Lee Seung-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1665344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Doing something “for the ‘gram” is not typical...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1665344</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Doing something “for the ‘gram” is not typical...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1668925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Harry Potter author JK Rowling has defended he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1668925</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Harry Potter author JK Rowling has defended he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1668925</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Harry Potter author JK Rowling has defended he...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1800363</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>More to come North Korea Asia Pacific news Sha...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entry_id  version  sent_idx  \\\n",
       "0    823187        0         0   \n",
       "0    823187        1         0   \n",
       "0   1665344        0         0   \n",
       "0   1665344        1         0   \n",
       "0   1668925        0         0   \n",
       "0   1668925        1         0   \n",
       "0   1668925        2         0   \n",
       "8   1800363        0         8   \n",
       "\n",
       "                                            sentence  deleted_label  \\\n",
       "0  South Korea’s much-loved K-pop star Lee Seung-...              1   \n",
       "0  South Korea’s much-loved K-pop star Lee Seung-...              1   \n",
       "0  Doing something “for the ‘gram” is not typical...              1   \n",
       "0  Doing something “for the ‘gram” is not typical...              1   \n",
       "0  Harry Potter author JK Rowling has defended he...              1   \n",
       "0  Harry Potter author JK Rowling has defended he...              1   \n",
       "0  Harry Potter author JK Rowling has defended he...              1   \n",
       "8  More to come North Korea Asia Pacific news Sha...              1   \n",
       "\n",
       "   add_above_label  add_below_label  edited_label  unchanged_label  \\\n",
       "0                0                0             0                1   \n",
       "0                0                0             0                1   \n",
       "0                0                0             0                1   \n",
       "0                0                0             0                1   \n",
       "0                0                0             0                1   \n",
       "0                0                0             0                1   \n",
       "0                0                0             0                1   \n",
       "8                2                0             1                0   \n",
       "\n",
       "   refactored_label  \n",
       "0                 0  \n",
       "0                 0  \n",
       "0                 0  \n",
       "0                 0  \n",
       "0                 0  \n",
       "0                 0  \n",
       "0                 0  \n",
       "8                 0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_sentence_labels_df\n",
    " .loc[lambda df: df[['edited_label', 'unchanged_label', 'deleted_label']].sum(axis=1).pipe(lambda s: s != 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence_labels_df.to_csv('../modeling/data/sentence-data-%s.csv' % source, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

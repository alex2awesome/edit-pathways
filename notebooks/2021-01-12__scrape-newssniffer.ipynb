{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument('--disable-gpu') \n",
    "\n",
    "driver = webdriver.Chrome(executable_path='/Users/alex/chromedriver', options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make chromedriver run on GCE\n",
    "os.environ['webdriver.chrome.driver'] = \"/usr/bin/chromedriver\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument('--disable-gpu') \n",
    "chrome_options.add_argument(\"start-maximized\")\n",
    "chrome_options.add_argument(\"disable-infobars\") \n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--no-sandbox\") \n",
    "driver = webdriver.Chrome(executable_path='/usr/bin/chromedriver', options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(path, use_selenium):\n",
    "    if use_selenium:\n",
    "        driver.get(path)\n",
    "        return driver.page_source\n",
    "    else:\n",
    "        page = requests.get(path)\n",
    "        return page.text\n",
    "\n",
    "def parse_table(soup):\n",
    "    table_output = []\n",
    "    for row in soup.find_all('tr'):\n",
    "        row_ouput = []\n",
    "        for cell in row.find_all('td'):\n",
    "            if cell.find('a'):\n",
    "                cell_link = cell.a['href']\n",
    "                row_ouput.append(cell_link)\n",
    "            cell_value = ''.join(cell.stripped_strings)\n",
    "            row_ouput.append(cell_value)\n",
    "        table_output.append(row_ouput)\n",
    "\n",
    "    table_df = (pd.DataFrame(\n",
    "        list(filter(lambda x: len(x) != 0, table_output)), \n",
    "        columns=['newssniff_link', 'headline', 'version', 'outlet', 'last_updated'])\n",
    "    )\n",
    "    return table_df\n",
    "\n",
    "def get_search_result(page, use_selenium=True):\n",
    "    url = 'https://www.newssniffer.co.uk/versions?page=%s' % page\n",
    "    search_page = get_html(url, use_selenium=True)\n",
    "    soup = BeautifulSoup(search_page.text)\n",
    "    page_df = parse_table(soup)\n",
    "    page_df['search_page'] = page\n",
    "    return page, page_df\n",
    "\n",
    "\n",
    "def parse_article(html):\n",
    "    \"\"\"Given the HTML of an article return a table with a row for each version.\"\"\"\n",
    "\n",
    "    ## get url\n",
    "    soup = BeautifulSoup(html)\n",
    "    source, url = soup.find_all('cite')[0].text, soup.find_all('cite')[1].text\n",
    "\n",
    "    ## get the rest of the article\n",
    "    version_tables = pd.read_html(html)[0]\n",
    "    version_table_flat = (version_tables\n",
    "     .apply(lambda s: ('</p><p>').join(s.dropna().tolist()))\n",
    "     .reset_index()\n",
    "     .rename(columns={'level_0': 'version', 'level_1':'title', 'level_2': 'time', 0:'text'})\n",
    "    )\n",
    "\n",
    "    ## merge\n",
    "    version_table_flat['url'] = url\n",
    "    version_table_flat['source'] = source\n",
    "\n",
    "    return version_table_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page = 239179\n",
    "start_page = 50503\n",
    "all_search_output = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as pool:        \n",
    "    result_futures = list(map(lambda x: pool.submit(get_search_result, x), range(start_page, last_page)))\n",
    "    for future in tqdm(concurrent.futures.as_completed(result_futures), total=last_page - start_page):\n",
    "        try:\n",
    "            page_idx, page_df = future.result()\n",
    "            all_search_output.append(page_df)\n",
    "            if len(all_search_output) > 1000:\n",
    "                all_search_output_df = pd.concat(all_search_output)\n",
    "                all_search_output_df.to_csv('cache/newssniffer-index-%s.csv' % page_idx)\n",
    "                all_search_output = []\n",
    "        except Exception as e:\n",
    "            print('e is', e, type(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(all_search_output).to_csv('cache/newssniffer-index-%s.csv' % page_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pages = list(map(lambda x: pd.read_csv(x, usecols=['search_page'], squeeze=True), search_files))\n",
    "searched_df = pd.concat(search_pages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = 200\n",
    "last_page = 239295\n",
    "all_search_pages = pd.Series(list(range(start_page, last_page)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-ids-scraped.csv      search-pages-scraped.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls ../scraping/scrapy-cloud/newssniffer_scrape/output_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_df.drop_duplicates().to_csv('../scraping/scrapy-cloud/newssniffer_scrape/output_dir/search-pages-scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_files = glob.glob('../scraping/scrapy-cloud/output_dir/newssniffer*')\n",
    "search_pages = list(map(lambda x: pd.read_csv(x, index_col=0), search_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newssniff_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>version</th>\n",
       "      <th>outlet</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>search_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1709190...</td>\n",
       "      <td>Fourth Soldier Dies After Coordinated Bomb Bla...</td>\n",
       "      <td>1</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>04 Dec 2018, 05:50</td>\n",
       "      <td>52505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1709318...</td>\n",
       "      <td>Arrests over India policeman killed by 'cow sl...</td>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>04 Dec 2018, 05:50</td>\n",
       "      <td>52505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1695611...</td>\n",
       "      <td>Left behind: families face uncertain future wh...</td>\n",
       "      <td>2</td>\n",
       "      <td>guardian</td>\n",
       "      <td>04 Dec 2018, 05:47</td>\n",
       "      <td>52505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1709331...</td>\n",
       "      <td>Country diary: thatching is a job to take your...</td>\n",
       "      <td>0</td>\n",
       "      <td>guardian</td>\n",
       "      <td>04 Dec 2018, 05:45</td>\n",
       "      <td>52505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1689299...</td>\n",
       "      <td>Angela Merkel’s successor could be bad news fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>guardian</td>\n",
       "      <td>04 Dec 2018, 05:45</td>\n",
       "      <td>52505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1680196...</td>\n",
       "      <td>Elizabeth Warren Releases DNA Results on Nativ...</td>\n",
       "      <td>0</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>15 Oct 2018, 13:55</td>\n",
       "      <td>56508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1680150...</td>\n",
       "      <td>Fracking starts at landmark Lancashire site</td>\n",
       "      <td>2</td>\n",
       "      <td>bbc</td>\n",
       "      <td>15 Oct 2018, 13:55</td>\n",
       "      <td>56508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1680175...</td>\n",
       "      <td>Cologne hostage taker ‘under control,’ one wom...</td>\n",
       "      <td>1</td>\n",
       "      <td>rtcom</td>\n",
       "      <td>15 Oct 2018, 13:55</td>\n",
       "      <td>56508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1679806...</td>\n",
       "      <td>Among the Ruins of Mexico Beach Stands One Hou...</td>\n",
       "      <td>3</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>15 Oct 2018, 13:55</td>\n",
       "      <td>56508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.newssniffer.co.uk/articles/1674689...</td>\n",
       "      <td>Banksy posts video of £1m painting shredding s...</td>\n",
       "      <td>10</td>\n",
       "      <td>bbc</td>\n",
       "      <td>15 Oct 2018, 13:50</td>\n",
       "      <td>56508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951872 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       newssniff_link  \\\n",
       "0   https://www.newssniffer.co.uk/articles/1709190...   \n",
       "1   https://www.newssniffer.co.uk/articles/1709318...   \n",
       "2   https://www.newssniffer.co.uk/articles/1695611...   \n",
       "3   https://www.newssniffer.co.uk/articles/1709331...   \n",
       "4   https://www.newssniffer.co.uk/articles/1689299...   \n",
       "..                                                ...   \n",
       "11  https://www.newssniffer.co.uk/articles/1680196...   \n",
       "12  https://www.newssniffer.co.uk/articles/1680150...   \n",
       "13  https://www.newssniffer.co.uk/articles/1680175...   \n",
       "14  https://www.newssniffer.co.uk/articles/1679806...   \n",
       "15  https://www.newssniffer.co.uk/articles/1674689...   \n",
       "\n",
       "                                             headline version    outlet  \\\n",
       "0   Fourth Soldier Dies After Coordinated Bomb Bla...       1   nytimes   \n",
       "1   Arrests over India policeman killed by 'cow sl...       1       bbc   \n",
       "2   Left behind: families face uncertain future wh...       2  guardian   \n",
       "3   Country diary: thatching is a job to take your...       0  guardian   \n",
       "4   Angela Merkel’s successor could be bad news fo...       1  guardian   \n",
       "..                                                ...     ...       ...   \n",
       "11  Elizabeth Warren Releases DNA Results on Nativ...       0   nytimes   \n",
       "12        Fracking starts at landmark Lancashire site       2       bbc   \n",
       "13  Cologne hostage taker ‘under control,’ one wom...       1     rtcom   \n",
       "14  Among the Ruins of Mexico Beach Stands One Hou...       3   nytimes   \n",
       "15  Banksy posts video of £1m painting shredding s...      10       bbc   \n",
       "\n",
       "          last_updated search_page  \n",
       "0   04 Dec 2018, 05:50       52505  \n",
       "1   04 Dec 2018, 05:50       52505  \n",
       "2   04 Dec 2018, 05:47       52505  \n",
       "3   04 Dec 2018, 05:45       52505  \n",
       "4   04 Dec 2018, 05:45       52505  \n",
       "..                 ...         ...  \n",
       "11  15 Oct 2018, 13:55       56508  \n",
       "12  15 Oct 2018, 13:55       56508  \n",
       "13  15 Oct 2018, 13:55       56508  \n",
       "14  15 Oct 2018, 13:55       56508  \n",
       "15  15 Oct 2018, 13:50       56508  \n",
       "\n",
       "[951872 rows x 6 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(search_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See output so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = list(map(\n",
    "    lambda x: pd.read_csv(x, index_col=0),\n",
    "    glob.glob('cache/newssniffer-index-*')\n",
    "))\n",
    "\n",
    "files_df = (pd.concat(files)\n",
    "            .reset_index(drop=True)\n",
    "            .assign(file_id=lambda df: df['newssniff_link'].str.split('/').str.get(4))\n",
    "           )\n",
    "\n",
    "max_diff_per_file_id = (\n",
    "    files_df[['version', 'file_id']]\n",
    "    .assign(version=lambda df: df['version'].astype(int))\n",
    "    .groupby('file_id')\n",
    "    ['version']    \n",
    "    .max()\n",
    ")\n",
    "\n",
    "files_df_versions = files_df.merge(\n",
    "    max_diff_per_file_id.astype(int).to_frame('num_diffs_per_file'),\n",
    "    left_on='file_id',\n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df['file_id'].value_counts().head()\n",
    "files_df.drop_duplicates('file_id')['outlet'].value_counts()\n",
    "files_df['last_updated_time'] = files_df['last_updated'].pipe(lambda s: pd.to_datetime(s, errors='coerce'))\n",
    "files_df['last_updated_time'].dropna().sort_values().min()\n",
    "files_df_versions.groupby('outlet')['num_diffs_per_file'].median()\n",
    "files_df_versions.drop_duplicates('file_id')['outlet'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Running the File fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cut, max_cut = (\n",
    "    files_df_versions\n",
    "        .drop_duplicates('file_id')['num_diffs_per_file']\n",
    "        .loc[lambda s: s != 0].quantile([.01, .99])\n",
    ")\n",
    "\n",
    "to_scrape = (files_df_versions\n",
    " .loc[lambda df: df['num_diffs_per_file'].pipe(lambda s: (s >= min_cut) & (s<=max_cut))] ## 1, 30\n",
    " [['file_id', 'num_diffs_per_file']]\n",
    " .drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dfs = []\n",
    "num_tries = 5\n",
    "start_at = 11194\n",
    "for loop_idx, (file_id, num_diffs) in tqdm(\n",
    "    enumerate(to_scrape.iloc[start_at:].itertuples(index=False)), \n",
    "    total=len(to_scrape.iloc[start_at:])\n",
    "):\n",
    "    if (loop_idx % 500 == 0) and len(article_dfs) > 0:\n",
    "        to_disk = pd.concat(article_dfs)\n",
    "        to_disk.to_csv('cache/article-versions-%s.csv' % (loop_idx + start_at))\n",
    "        article_dfs = []\n",
    "    \n",
    "    v = list(range(num_diffs+1))\n",
    "    for s, e in list(zip(v[:-1], v[1:])):\n",
    "        for i in range(num_tries):\n",
    "            try:\n",
    "                url = 'https://www.newssniffer.co.uk/articles/%s/diff/%s/%s' % (file_id, s, e)\n",
    "                html = get_html(url, use_selenium=True)\n",
    "                article_df = parse_article(html)\n",
    "                article_df['file_id'] = file_id\n",
    "                article_dfs.append(article_df)\n",
    "                break\n",
    "            except ConnectionError:\n",
    "                print('driver failed... re-initializing...')\n",
    "                driver.quit()\n",
    "                driver = webdriver.Chrome(executable_path='/Users/alex/chromedriver', options=chrome_options)\n",
    "            except:\n",
    "                print('failed, sleeping...')\n",
    "                time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article-Level Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "html = pd.read_html('https://www.newssniffer.co.uk/articles/2057481/diff/10/11')\n",
    "html[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "iterable = [1,2,3,4,6,7,8,9,10]\n",
    "\n",
    "def f(x):\n",
    "    if x == 2:\n",
    "        raise Exception('x')\n",
    "    return x\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "e = ThreadPoolExecutor(4)\n",
    "s = range(10)\n",
    "for i in e.map(time.sleep, s):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "\n",
    "class DemoSpider(scrapy.Spider):\n",
    "    name = 'demo'\n",
    "    start_urls = ['http://quotes.toscrape.com/js']\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DemoSpider, self).__init__(*args, **kwargs)\n",
    "\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        self.driver = webdriver.Chrome(chrome_options=options, executable_path='/usr/bin/chromedriver')\n",
    "    \n",
    "    def parse(self, response):\n",
    "        self.driver.get(response.url)\n",
    "        for quote in self.driver.find_elements_by_css_selector('div.quote'):\n",
    "            yield {\n",
    "                'quote': quote.find_element_by_css_selector('span').text,\n",
    "                'author': quote.find_element_by_css_selector('small').text,\n",
    "            }\n",
    "        next_page_url = response.css('nav li.next a ::attr(href)').extract_first()\n",
    "        if next_page_url:\n",
    "            yield scrapy.Request(response.urljoin(next_page_url))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

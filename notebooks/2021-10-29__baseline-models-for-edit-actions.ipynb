{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sqlite3\n",
    "import sys\n",
    "sys.path.insert(0, '../util')\n",
    "import util_refactorings as ur\n",
    "from importlib import reload\n",
    "reload(ur)\n",
    "import glob\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences_add(doc):\n",
    "    doc = doc.copy()\n",
    "    sent_idxs = doc['sent_idx_y'].dropna().sort_values().tolist()\n",
    "    additions = doc.loc[lambda df: df['sent_idx_x'].isnull()]['sent_idx_y'].tolist()\n",
    "    \n",
    "    add_aboves = []\n",
    "    add_belows = []\n",
    "    idx_in_add_l = 0\n",
    "    while idx_in_add_l < len(additions):\n",
    "        a = additions[idx_in_add_l]\n",
    "        idx_in_sent_l = sent_idxs.index(a)\n",
    "        cluster_size = 1\n",
    "        if idx_in_sent_l < len(sent_idxs) - cluster_size:\n",
    "            add_above = sent_idxs[idx_in_sent_l + cluster_size]\n",
    "            exists_sent_below = True\n",
    "            while add_above in additions:\n",
    "                cluster_size += 1\n",
    "                if (idx_in_sent_l + cluster_size) < len(sent_idxs):\n",
    "                    add_above = sent_idxs[idx_in_sent_l + cluster_size]\n",
    "                    exists_sent_below = True\n",
    "                else:\n",
    "                    exists_sent_below = False\n",
    "                    break\n",
    "            if exists_sent_below:\n",
    "                add_aboves.append({\n",
    "                    'add_above': add_above,\n",
    "                    'cluster_size': cluster_size\n",
    "                })\n",
    "        if idx_in_sent_l > 0:\n",
    "            add_below = sent_idxs[idx_in_sent_l - 1]\n",
    "            add_belows.append({\n",
    "                'add_below': add_below,\n",
    "                'cluster_size': cluster_size\n",
    "            })\n",
    "        idx_in_add_l += cluster_size\n",
    "    \n",
    "    return add_aboves, add_belows\n",
    "\n",
    "## label each sentence in the old version as:\n",
    "# 1. Deleted in the new version\n",
    "# 2. Sentence added above/sentence added below  \n",
    "# 3. Sentence edited\n",
    "# 4. Sentence refactored\n",
    "\n",
    "# 5. Sentence split (?)\n",
    "# 6. Sentence merge (?)\n",
    "\n",
    "def get_sentence_and_doc_labels(doc, doc_sentences):\n",
    "    # 1. Detect deletions:\n",
    "    deleted_labeled_sentences = pd.concat([\n",
    "        (doc_sentences\n",
    "         .loc[lambda df: ~df['sent_idx'].isin(doc['sent_idx_x'].dropna())]\n",
    "         .assign(deleted_label=True)\n",
    "         .rename(columns={'version':'version_x', 'sent_idx':'sent_idx_x'})\n",
    "         [['entry_id', 'version_x', 'sent_idx_x', 'deleted_label']]\n",
    "        )\n",
    "        ,\n",
    "        (doc\n",
    "         .loc[lambda df: df['sent_idx_y'].isnull()]\n",
    "          .assign(deleted_label=True)\n",
    "          [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'deleted_label']]\n",
    "        )\n",
    "    ]).drop_duplicates()\n",
    "\n",
    "    # 2. Sentence additions above/below\n",
    "    add_aboves, add_belows = label_sentences_add(doc)\n",
    "    if len(add_aboves) > 0:\n",
    "        add_above_labeled_sentences = (pd.DataFrame(add_aboves)\n",
    "        #  .assign(add_above_label=lambda df: df['cluster_size'].apply(lambda x: 'add above ' + str(x)))\n",
    "         .rename(columns={'cluster_size': 'add_above_label'})\n",
    "         .merge(doc, how='inner', right_on='sent_idx_y', left_on='add_above')\n",
    "         [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'add_above_label']]\n",
    "        )\n",
    "    else:\n",
    "        add_above_labeled_sentences = pd.DataFrame()\n",
    "        \n",
    "    if len(add_belows) > 0:\n",
    "        add_below_labeled_sentences = (pd.DataFrame(add_belows)\n",
    "        #  .assign(add_below_label=lambda df: df['cluster_size'].apply(lambda x: 'add below ' + str(x))) \n",
    "         .rename(columns={'cluster_size': 'add_below_label'})\n",
    "         .merge(doc, how='inner', right_on='sent_idx_y', left_on='add_below')\n",
    "         [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'add_below_label']]\n",
    "        )\n",
    "    else:\n",
    "        add_below_labeled_sentences = pd.DataFrame()\n",
    "#         doc['add_below_label'] = 0 \n",
    "\n",
    "    # 3. Sentence edits:\n",
    "    edited_sentences = (doc\n",
    "     .loc[lambda df: df['sent_idx_y'].notnull() & df['sent_idx_x'].notnull() & (df['avg_sentence_distance_x'] > .01)]\n",
    "     .assign(edited_label=True)\n",
    "      [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'edited_label']]\n",
    "    )\n",
    "    unchanged_sentences = (doc\n",
    "     .loc[lambda df: df['sent_idx_y'].notnull() & df['sent_idx_x'].notnull() & (df['avg_sentence_distance_x'] <= .01)]\n",
    "     .assign(unchanged_label=True)\n",
    "      [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'unchanged_label']]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 4. Sentence Refactors\n",
    "    refactors = ur.find_refactors_for_doc(doc)\n",
    "    refactored_sentences = (doc\n",
    "     .loc[lambda df: df.apply(lambda x: (x['sent_idx_x'], x['sent_idx_y']) in refactors, axis=1)]\n",
    "     .assign(refactored_label=lambda df: \n",
    "             df\n",
    "             .pipe(lambda df: df['sent_idx_y'] - df['sent_idx_x'])\n",
    "    #          .apply(lambda x: 'move %(direction)s %(num_steps)s' % ({\n",
    "    #              'direction': 'up' if x < 0 else 'down',\n",
    "    #              'num_steps': abs(int(x))\n",
    "    #              }))\n",
    "            )\n",
    "       [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'refactored_label']]\n",
    "    )\n",
    "\n",
    "    ## Concat to make Sentence-Level DF \n",
    "    sentence_label_df = (pd.concat([\n",
    "        deleted_labeled_sentences,\n",
    "        add_above_labeled_sentences,\n",
    "        add_below_labeled_sentences,\n",
    "        edited_sentences,\n",
    "        unchanged_sentences,\n",
    "        refactored_sentences,\n",
    "    ])\n",
    "     .assign(version_y=lambda df: df['version_y'].fillna(method='bfill'))\n",
    "     .fillna(False)\n",
    "     .astype(int)\n",
    "    )\n",
    "    if 'add_below_label' not in sentence_label_df:\n",
    "        sentence_label_df['add_below_label'] = 0\n",
    "    if 'add_above_label' not in sentence_label_df:\n",
    "        sentence_label_df['add_above_label'] = 0\n",
    "    \n",
    "    sentence_label_df = (sentence_label_df\n",
    "         .groupby(['entry_id', 'version_x', 'sent_idx_x'])\n",
    "         .agg({\n",
    "             'deleted_label': lambda s: max(s),\n",
    "             'add_above_label': lambda s: max(s),\n",
    "             'add_below_label': lambda s: max(s),\n",
    "             'edited_label': lambda s: max(s),\n",
    "             'unchanged_label': lambda s: max(s),\n",
    "             'refactored_label': lambda s: min(s)\n",
    "         })\n",
    "         .reset_index()\n",
    "        )\n",
    "    \n",
    "    sentence_label_df = doc_sentences.merge(\n",
    "            sentence_label_df,\n",
    "            how='left',\n",
    "            left_on=['entry_id', 'version', 'sent_idx'],\n",
    "            right_on=['entry_id', 'version_x', 'sent_idx_x']\n",
    "        ).drop(['version_x', 'sent_idx_x'], axis=1).fillna(0)\n",
    "        \n",
    "    ## Make Doc-Label DF\n",
    "    doc_label_df = (sentence_label_df\n",
    "     .assign(refactored_label=lambda df: (df['refactored_label'] != 0).astype(int))\n",
    "     .groupby(['entry_id', 'version'])\n",
    "     .aggregate({\n",
    "         'deleted_label':'sum',\n",
    "         'add_above_label':'sum',\n",
    "         'edited_label': 'sum',\n",
    "         'refactored_label': 'sum',\n",
    "         'sentence': lambda s: '<SENT>'.join(s)\n",
    "     })\n",
    "     .rename(columns={\n",
    "         'deleted_label': 'num_deleted',\n",
    "         'add_above_label': 'num_added',\n",
    "         'edited_label': 'num_edited',\n",
    "         'refactored_label': 'num_refactored',\n",
    "         'sentence': 'sentences'\n",
    "     })\n",
    "    )   \n",
    "    \n",
    "    return sentence_label_df, doc_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_and_matched_dfs(conn, sents_max=30, sents_min=3):\n",
    "    low_count_versions = pd.read_sql('''\n",
    "    with c1 as \n",
    "        (SELECT entry_id, \n",
    "            CAST(version as INT) as version, \n",
    "            COUNT(1) as c from split_sentences \n",
    "            GROUP BY entry_id, version)\n",
    "    SELECT entry_id, version from c1\n",
    "    WHERE c < %s and c > %s\n",
    "    '''% (sents_max, sents_min), con=conn)\n",
    "\n",
    "    # get join keys\n",
    "    low_count_entry_ids = ', '.join(list(map(str, low_count_versions['entry_id'].unique())))\n",
    "    joint_keys = low_count_versions.pipe(lambda df: df['entry_id'].astype(str) + '-' + df['version'].astype(str))\n",
    "    joint_keys = \"'%s'\" % \"', '\".join(joint_keys.tolist())\n",
    "\n",
    "    # matched sentences\n",
    "    matched_sentences = pd.read_sql('''\n",
    "        WITH c1 as ( \n",
    "        SELECT *, \n",
    "        entry_id || '-' || version_x as key_x,\n",
    "        entry_id || '-' || version_y as key_y \n",
    "        FROM matched_sentences \n",
    "        )\n",
    "        SELECT *\n",
    "        FROM c1\n",
    "        WHERE key_x in (%s) AND key_y  in (%s)\n",
    "        ''' % (joint_keys, joint_keys)\n",
    "    , con=conn)\n",
    "\n",
    "    # get split sentences\n",
    "    split_sentences = pd.read_sql('''\n",
    "        with c1 AS (\n",
    "            SELECT *, entry_id || '-' || CAST(version AS INT) as key FROM split_sentences\n",
    "        )\n",
    "        SELECT entry_id, CAST(version AS INT) as version, sent_idx, sentence \n",
    "        FROM c1\n",
    "        WHERE key IN (%s)\n",
    "    ''' % joint_keys, con=conn)\n",
    "    return matched_sentences, split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/diffengine-diffs/spark-output/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/diffengine-diffs/spark-output/independent-matched-sentences.db',\n",
       " '../data/diffengine-diffs/spark-output/reuters-matched-sentences.db',\n",
       " '../data/diffengine-diffs/spark-output/guardian-matched-sentences.db',\n",
       " '../data/diffengine-diffs/spark-output/nyt-matched-sentences.db',\n",
       " '../data/diffengine-diffs/spark-output/bbc-2-matched-sentences.db',\n",
       " '../data/diffengine-diffs/spark-output/ap-matched-sentences.db']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching data for independent ...\n",
      "calculating statistics...\n",
      "processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d592f56a5c134992881f1165806f0cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to disk...\n",
      "fetching data for reuters ...\n",
      "calculating statistics...\n",
      "processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df25f84d6af48a6a676fda56e10fa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to disk...\n",
      "fetching data for guardian ...\n",
      "calculating statistics...\n",
      "processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640e9b7019ab411baa3dbd29d94bec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in files[1:]:\n",
    "    source = f.split('/')[-1].split('-')[0]\n",
    "    conn = sqlite3.connect(f)\n",
    "    print('fetching data for %s ...' % source)\n",
    "    matched_sentences, split_sentences = get_split_and_matched_dfs(conn)\n",
    "\n",
    "    print('calculating statistics...')\n",
    "    edit_statistics = (matched_sentences\n",
    "     .groupby(['entry_id', 'version_x', 'version_y'])\n",
    "     .apply(lambda df: pd.Series({\n",
    "         'mean x dist': df['avg_sentence_distance_x'].mean(),\n",
    "         'mean y dist': df['avg_sentence_distance_y'].mean(),\n",
    "         'num_deleted' : df['sent_idx_y'].isnull().sum(),\n",
    "         'num_added' : df['sent_idx_x'].isnull().sum(),\n",
    "         'refactors': ur.find_refactors_for_doc(\n",
    "             df[['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'sent_idx_y']].dropna().astype(int)\n",
    "         ),\n",
    "     }))\n",
    "     .assign(num_refactors=lambda df: df['refactors'].str.len())\n",
    "     .assign(overall_mean=lambda df: df[['mean x dist', 'mean y dist']].mean(axis=1))\n",
    "    )\n",
    "\n",
    "    desired_docs = (edit_statistics\n",
    "     .loc[lambda df:\n",
    "          (df['overall_mean'] > .01) |\n",
    "          (df['num_deleted'] > 0) |\n",
    "          (df['num_added'] > 0) |\n",
    "          (df['num_refactors'] > 0)]\n",
    "    )\n",
    "\n",
    "\n",
    "    print('processing documents...')\n",
    "    all_sentence_labels = []\n",
    "    all_doc_labels = []\n",
    "    # for sanity checking\n",
    "    all_matched_sentences = []\n",
    "    all_split_sentences = []\n",
    "\n",
    "    for entry_id, v_x, v_y in tqdm(desired_docs.index):\n",
    "        doc = (matched_sentences\n",
    "         .loc[lambda df: (df['entry_id'] == entry_id) & (df['version_x'] == v_x) & (df['version_y'] == v_y)]\n",
    "         .sort_values(['sent_idx_x', 'sent_idx_y'])\n",
    "        )\n",
    "\n",
    "        doc_sentences = (split_sentences\n",
    "         .loc[lambda df: (df['entry_id'] == entry_id) & (df['version'] == v_x) ]\n",
    "                         .sort_values('sent_idx')\n",
    "        )\n",
    "\n",
    "        all_matched_sentences.append(doc)\n",
    "        all_split_sentences.append(doc_sentences)\n",
    "        sentence_label_df, doc_label_df = get_sentence_and_doc_labels(doc, doc_sentences)\n",
    "        all_sentence_labels.append(sentence_label_df)\n",
    "        all_doc_labels.append(doc_label_df)\n",
    "\n",
    "    print('writing to disk...')\n",
    "    all_doc_labels_df = pd.concat(all_doc_labels)\n",
    "    all_doc_labels_df.to_csv('../modeling/data/doc-data-%s.csv' % source, index=False)\n",
    "    all_sentence_labels_df = pd.concat(all_sentence_labels)\n",
    "    ## check \n",
    "    assert (all_sentence_labels_df[['edited_label', 'unchanged_label', 'deleted_label']]\n",
    "            .sum(axis=1)\n",
    "            .pipe(lambda s: s == 1)\n",
    "            .all()\n",
    "           )\n",
    "    all_sentence_labels_df.to_csv('../modeling/data/sentence-data-%s.csv' % source, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

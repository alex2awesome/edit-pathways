{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://artifactory.inf.bloomberg.com/artifactory/api/pypi/bloomberg-pypi/simple/, https://artifactory.inf.bloomberg.com/artifactory/api/pypi/python-dsp-wheels/simple\n",
      "Requirement already satisfied: more_itertools in /job/.local/lib/python3.7/site-packages (8.10.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://artifactory.inf.bloomberg.com/artifactory/api/pypi/bloomberg-pypi/simple/, https://artifactory.inf.bloomberg.com/artifactory/api/pypi/python-dsp-wheels/simple\n",
      "Collecting tqdm\n",
      "  Downloading https://artifactory.inf.bloomberg.com/artifactory/api/pypi/bloomberg-pypi/packages/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "! pip install more_itertools\n",
    "! pip install tqdm\n",
    "! pip install ipywidgets\n",
    "! pip install jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sqlite3\n",
    "import sys\n",
    "sys.path.insert(0, '../util')\n",
    "import util_refactorings as ur\n",
    "from importlib import reload\n",
    "reload(ur)\n",
    "import glob\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences_add(doc):\n",
    "    doc = doc.copy()\n",
    "    sent_idxs = doc['sent_idx_y'].dropna().sort_values().tolist()\n",
    "    additions = doc.loc[lambda df: df['sent_idx_x'].isnull()]['sent_idx_y'].tolist()\n",
    "    \n",
    "    add_aboves = []\n",
    "    add_belows = []\n",
    "    idx_in_add_l = 0\n",
    "    while idx_in_add_l < len(additions):\n",
    "        a = additions[idx_in_add_l]\n",
    "        idx_in_sent_l = sent_idxs.index(a)\n",
    "        cluster_size = 1\n",
    "        if idx_in_sent_l < len(sent_idxs) - cluster_size:\n",
    "            add_above = sent_idxs[idx_in_sent_l + cluster_size]\n",
    "            exists_sent_below = True\n",
    "            while add_above in additions:\n",
    "                cluster_size += 1\n",
    "                if (idx_in_sent_l + cluster_size) < len(sent_idxs):\n",
    "                    add_above = sent_idxs[idx_in_sent_l + cluster_size]\n",
    "                    exists_sent_below = True\n",
    "                else:\n",
    "                    exists_sent_below = False\n",
    "                    break\n",
    "            if exists_sent_below:\n",
    "                add_aboves.append({\n",
    "                    'add_above': add_above,\n",
    "                    'cluster_size': cluster_size\n",
    "                })\n",
    "        if idx_in_sent_l > 0:\n",
    "            add_below = sent_idxs[idx_in_sent_l - 1]\n",
    "            add_belows.append({\n",
    "                'add_below': add_below,\n",
    "                'cluster_size': cluster_size\n",
    "            })\n",
    "        idx_in_add_l += cluster_size\n",
    "    \n",
    "    return add_aboves, add_belows\n",
    "\n",
    "## label each sentence in the old version as:\n",
    "# 1. Deleted in the new version\n",
    "# 2. Sentence added above/sentence added below  \n",
    "# 3. Sentence edited\n",
    "# 4. Sentence refactored\n",
    "\n",
    "# 5. Sentence split (?)\n",
    "# 6. Sentence merge (?)\n",
    "\n",
    "def get_sentence_and_doc_labels(doc):\n",
    "    # 1. Detect deletions:\n",
    "#     doc_sentences = (\n",
    "#         pd.DataFrame({'sent_idx_x': doc.iloc[0]['all_sents_in_split_sents']})\n",
    "#              .assign(entry_id=doc['entry_id'].iloc[0])\n",
    "#              .assign(version_x=doc['version_x'].iloc[0])\n",
    "#     )\n",
    "    doc = doc.sort_values(['sent_idx_x', 'sent_idx_y'])\n",
    "    deleted_labeled_sentences = ( # pd.concat([\n",
    "#         (doc_sentences\n",
    "#          .loc[lambda df: ~df['sent_idx_x'].isin(doc['sent_idx_x'].dropna())]\n",
    "#          .assign(deleted_label=True)\n",
    "#     #      [['entry_id', 'version_x', 'sent_idx_x', 'deleted_label']]\n",
    "#         )\n",
    "#         ,\n",
    "        doc\n",
    "         .loc[lambda df: df['sent_idx_y'].isnull()]\n",
    "          .assign(deleted_label=True)\n",
    "          [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'deleted_label']]\n",
    "        )\n",
    "#     ]).drop_duplicates()\n",
    "\n",
    "    # 2. Sentence additions above/below\n",
    "    add_aboves, add_belows = label_sentences_add(doc)\n",
    "    if len(add_aboves) > 0:\n",
    "        add_above_labeled_sentences = (pd.DataFrame(add_aboves)\n",
    "        #  .assign(add_above_label=lambda df: df['cluster_size'].apply(lambda x: 'add above ' + str(x)))\n",
    "         .rename(columns={'cluster_size': 'add_above_label'})\n",
    "         .merge(doc, how='inner', right_on='sent_idx_y', left_on='add_above')\n",
    "         [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'add_above_label']]\n",
    "        )\n",
    "    else:\n",
    "        add_above_labeled_sentences = pd.DataFrame()\n",
    "        \n",
    "    if len(add_belows) > 0:\n",
    "        add_below_labeled_sentences = (pd.DataFrame(add_belows)\n",
    "        #  .assign(add_below_label=lambda df: df['cluster_size'].apply(lambda x: 'add below ' + str(x))) \n",
    "         .rename(columns={'cluster_size': 'add_below_label'})\n",
    "         .merge(doc, how='inner', right_on='sent_idx_y', left_on='add_below')\n",
    "         [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'add_below_label']]\n",
    "        )\n",
    "    else:\n",
    "        add_below_labeled_sentences = pd.DataFrame()\n",
    "#         doc['add_below_label'] = 0 \n",
    "\n",
    "    # 3. Sentence edits:\n",
    "    edited_sentences = (doc\n",
    "     .loc[lambda df: df['sent_idx_y'].notnull() & df['sent_idx_x'].notnull() & (df['avg_sentence_distance_x'] > .01)]\n",
    "     .assign(edited_label=True)\n",
    "      [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'edited_label']]\n",
    "    )\n",
    "    unchanged_sentences = (doc\n",
    "     .loc[lambda df: df['sent_idx_y'].notnull() & df['sent_idx_x'].notnull() & (df['avg_sentence_distance_x'] <= .01)]\n",
    "     .assign(unchanged_label=True)\n",
    "      [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'unchanged_label']]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 4. Sentence Refactors\n",
    "#     refactors = ur.find_refactors_for_doc(doc)\n",
    "    refactors = doc['refactors'].iloc[0]\n",
    "    refactored_sentences = (doc\n",
    "     .loc[lambda df: df.apply(lambda x: (x['sent_idx_x'], x['sent_idx_y']) in refactors, axis=1)]\n",
    "     .assign(refactored_label=lambda df: \n",
    "             df\n",
    "             .pipe(lambda df: df['sent_idx_y'] - df['sent_idx_x'])\n",
    "    #          .apply(lambda x: 'move %(direction)s %(num_steps)s' % ({\n",
    "    #              'direction': 'up' if x < 0 else 'down',\n",
    "    #              'num_steps': abs(int(x))\n",
    "    #              }))\n",
    "            )\n",
    "       [['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'refactored_label']]\n",
    "    )\n",
    "\n",
    "    ## Concat to make Sentence-Level DF \n",
    "    sentence_label_df = (pd.concat([\n",
    "        deleted_labeled_sentences,\n",
    "        add_above_labeled_sentences,\n",
    "        add_below_labeled_sentences,\n",
    "        edited_sentences,\n",
    "        unchanged_sentences,\n",
    "        refactored_sentences,\n",
    "    ])\n",
    "     .assign(version_y=lambda df: df['version_y'].fillna(method='bfill'))\n",
    "     .fillna(False)\n",
    "     .astype(int)\n",
    "    )\n",
    "    if 'add_below_label' not in sentence_label_df:\n",
    "        sentence_label_df['add_below_label'] = 0\n",
    "    if 'add_above_label' not in sentence_label_df:\n",
    "        sentence_label_df['add_above_label'] = 0\n",
    "    \n",
    "    sentence_label_df = (sentence_label_df\n",
    "         .groupby(['entry_id', 'version_x', 'sent_idx_x'])\n",
    "         .agg({\n",
    "             'deleted_label': lambda s: max(s),\n",
    "             'add_above_label': lambda s: max(s),\n",
    "             'add_below_label': lambda s: max(s),\n",
    "             'edited_label': lambda s: max(s),\n",
    "             'unchanged_label': lambda s: max(s),\n",
    "             'refactored_label': lambda s: min(s)\n",
    "         })\n",
    "         .reset_index()\n",
    "        )\n",
    "    \n",
    "#     sentence_label_df = doc_sentences.merge(\n",
    "#             sentence_label_df,\n",
    "#             how='left',\n",
    "#             left_on=['entry_id', 'version_x', 'sent_idx_x'],\n",
    "#             right_on=['entry_id', 'version_x', 'sent_idx_x']\n",
    "#         ).rename(columns={'version_x': 'version', 'sent_idx_x':'sent_idx'}).fillna(0)\n",
    "        \n",
    "    ## Make Doc-Label DF\n",
    "#     doc_label_df = (sentence_label_df\n",
    "#      .assign(refactored_label=lambda df: (df['refactored_label'] != 0).astype(int))\n",
    "#      .groupby(['entry_id', 'version'])\n",
    "#      .aggregate({\n",
    "#          'deleted_label':'sum',\n",
    "#          'add_above_label':'sum',\n",
    "#          'edited_label': 'sum',\n",
    "#          'refactored_label': 'sum',\n",
    "#          'sentence': lambda s: '<SENT>'.join(s)\n",
    "#      })\n",
    "#      .rename(columns={\n",
    "#          'deleted_label': 'num_deleted',\n",
    "#          'add_above_label': 'num_added',\n",
    "#          'edited_label': 'num_edited',\n",
    "#          'refactored_label': 'num_refactored',\n",
    "#          'sentence': 'sentences'\n",
    "#      })\n",
    "#     )   \n",
    "    \n",
    "    return sentence_label_df#, doc_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_and_matched_dfs(conn, sents_max=30, sents_min=3):\n",
    "    low_count_versions = pd.read_sql('''\n",
    "    with c1 as (\n",
    "        SELECT entry_id, \n",
    "            CAST(version as INT) as version, \n",
    "            COUNT(1) as c from split_sentences \n",
    "            GROUP BY entry_id, version\n",
    "    )\n",
    "    SELECT entry_id, version from c1\n",
    "        WHERE c < %s and c > %s\n",
    "    '''% (sents_max, sents_min), con=conn)\n",
    "\n",
    "    # get join keys\n",
    "    low_count_entry_ids = ', '.join(list(map(str, low_count_versions['entry_id'].unique())))\n",
    "    joint_keys = low_count_versions.pipe(lambda df: df['entry_id'].astype(str) + '-' + df['version'].astype(str))\n",
    "    joint_keys = \"'%s'\" % \"', '\".join(joint_keys.tolist())\n",
    "\n",
    "    # matched sentences\n",
    "    matched_sentences = pd.read_sql('''\n",
    "        WITH c1 as ( \n",
    "        SELECT *, \n",
    "        entry_id || '-' || version_x as key_x,\n",
    "        entry_id || '-' || version_y as key_y \n",
    "        FROM matched_sentences \n",
    "        )\n",
    "        SELECT *\n",
    "        FROM c1\n",
    "        WHERE key_x in (%s) AND key_y  in (%s)\n",
    "        ''' % (joint_keys, joint_keys)\n",
    "    , con=conn)\n",
    "\n",
    "    # get split sentences\n",
    "    split_sentences = pd.read_sql('''\n",
    "        with c1 AS (\n",
    "            SELECT *, entry_id || '-' || CAST(version AS INT) as key FROM split_sentences\n",
    "        )\n",
    "        SELECT entry_id, CAST(version AS INT) as version, sent_idx, sentence \n",
    "        FROM c1\n",
    "        WHERE key IN (%s)\n",
    "    ''' % joint_keys, con=conn)\n",
    "    return matched_sentences, split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))\n",
    "    return retLst\n",
    "\n",
    "def perform_one_statistics(df):\n",
    "    s = pd.Series({\n",
    "             'entry_id': df['entry_id'].iloc[0],\n",
    "             'version_x': df['version_x'].iloc[0],\n",
    "             'version_y': df['version_y'].iloc[0],\n",
    "             'mean x dist': df['avg_sentence_distance_x'].mean(),\n",
    "             'mean y dist': df['avg_sentence_distance_y'].mean(),\n",
    "             'num_deleted' : df['sent_idx_y'].isnull().sum(),\n",
    "             'num_added' : df['sent_idx_x'].isnull().sum(),\n",
    "             'refactors': ur.find_refactors_for_doc(\n",
    "                 df[['entry_id', 'version_x', 'version_y', 'sent_idx_x', 'sent_idx_y']].dropna().astype(int)\n",
    "             ),\n",
    "         })\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-23 22:24:18  183057701 ap-matched-sentences.db.gz\n",
      "2021-05-27 18:35:03 1314133314 bbc-2-matched-sentences.db.gz\n",
      "2021-06-03 01:45:37 2510969115 guardian-matched-sentences.db.gz\n",
      "2021-06-03 05:38:20  268104070 independent-matched-sentences.db.gz\n",
      "2021-06-23 18:06:17 1523600500 nyt-matched-sentences.db.gz\n",
      "2021-06-23 22:52:52  292799523 reuters-matched-sentences.db.gz\n",
      "2021-06-03 04:23:46  227431740 wp-matched-sentences.db.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls s3://aspangher/edit-pathways/spark_output_final/ --endpoint http://s3.dev.obdc.bcs.bloomberg.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aspangher/edit-pathways/spark_output_final/guardian-matched-sentences.db.gz to ./guardian-matched-sentences.db.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://aspangher/edit-pathways/spark_output_final/guardian-matched-sentences.db.gz . --endpoint http://s3.dev.obdc.bcs.bloomberg.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip guardian-matched-sentences.db.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/diffengine-diffs/spark-output/*')\n",
    "files = glob.glob('*matched-sentences.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bbc-2-matched-sentences.db',\n",
       " 'ap-matched-sentences.db',\n",
       " 'nyt-matched-sentences.db',\n",
       " 'guardian-matched-sentences.db']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching data for guardian ...\n"
     ]
    }
   ],
   "source": [
    "source = f.split('/')[-1].split('-')[0]\n",
    "conn = sqlite3.connect(f)\n",
    "print('fetching data for %s ...' % source)\n",
    "matched_sentences, split_sentences = get_split_and_matched_dfs(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd8eeded15a424ab95cc1d652567d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('calculating statistics...')\n",
    "edit_statistics = applyParallel(\n",
    "    matched_sentences.groupby(['entry_id', 'version_x', 'version_y']),\n",
    "    perform_one_statistics\n",
    ")\n",
    "\n",
    "print('creating dataframe...')\n",
    "edit_statistics = pd.concat(edit_statistics, axis=1).T.set_index(['entry_id', 'version_x', 'version_y'])\n",
    "edit_statistics = (edit_statistics             \n",
    " .assign(num_refactors=lambda df: df['refactors'].str.len())\n",
    " .assign(overall_mean=lambda df: df[['mean x dist', 'mean y dist']].mean(axis=1))\n",
    ")\n",
    "\n",
    "print('getting desired docs...')\n",
    "desired_docs = (edit_statistics\n",
    " .loc[lambda df:\n",
    "      (df['overall_mean'] > .01) |\n",
    "      (df['num_deleted'] > 0) |\n",
    "      (df['num_added'] > 0) |\n",
    "      (df['num_refactors'] > 0)]\n",
    ")\n",
    "\n",
    "print('merging...')\n",
    "sents_to_process = (\n",
    "    matched_sentences\n",
    "        .merge(\n",
    "        desired_docs['refactors'].reset_index(),\n",
    "        left_on=['entry_id', 'version_x', 'version_y'],\n",
    "        right_on=['entry_id', 'version_x', 'version_y']\n",
    "    )\n",
    "        .merge(\n",
    "            split_sentences.groupby(['entry_id', 'version'])['sent_idx'].aggregate(list).reset_index(),\n",
    "            left_on=['entry_id', 'version_x'],\n",
    "            right_on=['entry_id', 'version'],\n",
    "            how='left'\n",
    "    )\n",
    "    .drop(['version'], axis=1)\n",
    "    .rename(columns={'sent_idx': 'all_sents_in_split_sents'})\n",
    ")\n",
    "\n",
    "print('getting sentence labels...')\n",
    "output = applyParallel(\n",
    "    sents_to_process.groupby(['entry_id', 'version_x', 'version_y']),\n",
    "    get_sentence_and_doc_labels\n",
    ")\n",
    "\n",
    "print('getting doc labels...')\n",
    "split_sents_desired = (\n",
    "    split_sentences\n",
    "       .merge(\n",
    "           desired_docs.reset_index()[['entry_id', 'version_x']], \n",
    "           left_on=['entry_id', 'version'], \n",
    "           right_on=['entry_id', 'version_x']\n",
    "       )\n",
    ")\n",
    "\n",
    "split_sents_desired = split_sents_desired.drop(['version_x'], axis=1)\n",
    "output_df = (\n",
    "    pd.concat(output)\n",
    "     .rename(columns={'version_x':'version', 'sent_idx_x': 'sent_idx'})\n",
    ")\n",
    "\n",
    "output_df = (\n",
    "    output_df\n",
    "     .merge(split_sents_desired, left_on=['entry_id', 'version', 'sent_idx'], right_on=['entry_id', 'version', 'sent_idx'], how='outer')\n",
    "     .assign(deleted_label=lambda df: df['deleted_label'].fillna(1))\n",
    "     .fillna(0)\n",
    ")\n",
    "\n",
    "doc_changes = output_df.groupby(['entry_id', 'version']).agg({\n",
    "    'deleted_label': 'sum', \n",
    "    'add_above_label': 'sum', \n",
    "    'add_below_label': 'sum', \n",
    "    'edited_label': 'sum',\n",
    "    'unchanged_label': 'sum',\n",
    "    'refactored_label': lambda s: (s>0).sum(),\n",
    "    'sentence': '<SENT>'.join\n",
    "}).rename(columns={\n",
    "    'deleted_label': 'num_deleted', \n",
    "    'add_below_label': 'num_added', \n",
    "    'edited_label': 'num_edited',\n",
    "    'unchanged_label': 'num_unchanged',\n",
    "    'refactored_label': 'num_refactored',\n",
    "    'sentence': 'sentences'\n",
    "}).reset_index()\n",
    "\n",
    "output_df.to_csv('sentence-data-%s.csv' % source, index=False)\n",
    "doc_changes.to_csv('doc-data-%s.csv' % source, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('processing documents...')\n",
    "all_sentence_labels = []\n",
    "all_doc_labels = []\n",
    "\n",
    "for entry_id, v_x, v_y in tqdm(desired_docs.index):\n",
    "    doc = (matched_sentences\n",
    "     .loc[lambda df: (df['entry_id'] == entry_id) & (df['version_x'] == v_x) & (df['version_y'] == v_y)]\n",
    "     .sort_values(['sent_idx_x', 'sent_idx_y'])\n",
    "    )\n",
    "\n",
    "    doc_sentences = (split_sentences\n",
    "     .loc[lambda df: (df['entry_id'] == entry_id) & (df['version'] == v_x) ]\n",
    "                     .sort_values('sent_idx')\n",
    "    )\n",
    "\n",
    "    sentence_label_df, doc_label_df = get_sentence_and_doc_labels(doc, doc_sentences)\n",
    "    all_sentence_labels.append(sentence_label_df)\n",
    "    all_doc_labels.append(doc_label_df)\n",
    "\n",
    "print('writing to disk...')\n",
    "all_doc_labels_df = pd.concat(all_doc_labels)\n",
    "all_doc_labels_df.to_csv('../modeling/data/doc-data-%s.csv' % source, index=False)\n",
    "all_sentence_labels_df = pd.concat(all_sentence_labels)\n",
    "## check \n",
    "assert (all_sentence_labels_df[['edited_label', 'unchanged_label', 'deleted_label']]\n",
    "        .sum(axis=1)\n",
    "        .pipe(lambda s: s == 1)\n",
    "        .all()\n",
    "       )\n",
    "all_sentence_labels_df.to_csv('../modeling/data/sentence-data-%s.csv' % source, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

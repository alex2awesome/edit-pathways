{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Data for Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matched_sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>split_sentences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name\n",
       "0  matched_sentences\n",
       "1    split_sentences"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from util import util_refactorings as ur\n",
    "import sqlite3\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "db_filename = '../data/diffengine-diffs/spark-output/nyt-matched-sentences.db'\n",
    "\n",
    "if not os.path.exists(db_filename):\n",
    "    db_zip = db_filename + '.gz'\n",
    "    ! gunzip $db_zip\n",
    "\n",
    "conn = sqlite3.connect(db_filename)\n",
    "pd.read_sql('''SELECT \n",
    "                    name\n",
    "                FROM \n",
    "                    sqlite_master \n",
    "                WHERE \n",
    "                    type ='table' AND \n",
    "                    name NOT LIKE 'sqlite_%';\n",
    "''', con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_count_versions = pd.read_sql('''\n",
    "    with c1 as \n",
    "        (SELECT entry_id, version, COUNT(1) as c from split_sentences GROUP BY entry_id, version)\n",
    "    SELECT entry_id, version from c1\n",
    "    WHERE c < 10 and c > 5\n",
    "''', con=conn)\n",
    "\n",
    "# get join keys\n",
    "low_count_entry_ids = ', '.join(list(map(str, low_count_versions['entry_id'].unique())))\n",
    "joint_keys = low_count_versions.pipe(lambda df: df['entry_id'].astype(str) + '-' + df['version'].astype(str))\n",
    "joint_keys = \"'%s'\" % \"', '\".join(joint_keys.tolist())\n",
    "\n",
    "# matched sentences\n",
    "matched_sentences = pd.read_sql('''\n",
    "    WITH c1 as ( \n",
    "    SELECT *, \n",
    "    entry_id || '-' || version_x as key_x,\n",
    "    entry_id || '-' || version_y as key_y \n",
    "    FROM matched_sentences \n",
    "    )\n",
    "    SELECT *\n",
    "    FROM c1\n",
    "    WHERE key_x in (%s) AND key_y  in (%s)\n",
    "    ''' % (joint_keys, joint_keys)\n",
    ", con=conn)\n",
    "\n",
    "# get split sentences\n",
    "split_sentences = pd.read_sql('''\n",
    "    with c1 AS (\n",
    "        SELECT *, entry_id || '-' || version as key FROM split_sentences\n",
    "    )\n",
    "    SELECT * from c1\n",
    "    WHERE key IN (%s)\n",
    "''' % joint_keys, con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arcs_dict = matched_sentences.to_dict(orient='rows')\n",
    "\n",
    "# get HTML diffs\n",
    "doc_arcs = (matched_sentences\n",
    " .merge(split_sentences, how='outer', \n",
    "              right_on=['entry_id', 'version', 'sent_idx'],\n",
    "              left_on=['entry_id', 'version_x', 'sent_idx_x'] ,\n",
    "  ).drop(['version', 'sent_idx'], axis=1)\n",
    " .merge(split_sentences, how='outer', \n",
    "              right_on=['entry_id', 'version', 'sent_idx'],\n",
    "              left_on=['entry_id', 'version_y', 'sent_idx_y'] ,\n",
    "  ).drop(['version', 'sent_idx'], axis=1) \n",
    ")\n",
    "\n",
    "grouped_arcs = (matched_sentences\n",
    " .groupby(['entry_id', 'version_x', 'version_y'])\n",
    " .apply(lambda df: df[['version_x', 'version_y', 'sent_idx_x', 'sent_idx_y']].to_dict(orient='rows'))\n",
    " .to_frame('arcs')\n",
    ")\n",
    "\n",
    "grouped_nodes = (split_sentences\n",
    " .groupby(['entry_id', 'version'])\n",
    " .apply(lambda df: df[['version', 'sent_idx', 'sentence']].to_dict(orient='rows'))\n",
    ").to_frame('nodes').reset_index()\n",
    "\n",
    "matched_grouped_nodes = (grouped_nodes\n",
    " .merge(\n",
    "     grouped_nodes.assign(next_vers=lambda df: df['version'] - 1), \n",
    "     left_on=['entry_id', 'version'], \n",
    "     right_on=['entry_id', 'next_vers']\n",
    " )\n",
    " .assign(nodes=lambda df: df['nodes_x'] + df['nodes_y'])\n",
    " [['entry_id', 'version_x', 'version_y', 'nodes']]\n",
    " .set_index(['entry_id', 'version_x', 'version_y'])\n",
    ")\n",
    "\n",
    "output = (\n",
    "    pd.concat([matched_grouped_nodes, grouped_arcs], axis=1)\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "output = {str(k): v for k, v in output.items()}\n",
    "\n",
    "import json\n",
    "with open('../evaluation/data/sample_datum_small.json', 'w') as f:\n",
    "    json.dump(output, f )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both\n",
    "merged_matched_sentences = (matched_sentences\n",
    " .merge(\n",
    "    split_sentences, left_on=['entry_id', 'version_x', 'sent_idx_x'], right_on=['entry_id', 'version', 'sent_idx'],\n",
    "    how='left'\n",
    " ).drop(['version', 'sent_idx', 'key', 'key_x', 'key_y'], axis=1)\n",
    " .merge(\n",
    "    split_sentences, left_on=['entry_id', 'version_y', 'sent_idx_y'], right_on=['entry_id', 'version', 'sent_idx'],\n",
    "    how='left'\n",
    " ).drop(['version', 'sent_idx', 'key',], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>version_x</th>\n",
       "      <th>version_y</th>\n",
       "      <th>sent_idx_x</th>\n",
       "      <th>sent_idx_y</th>\n",
       "      <th>avg_sentence_distance_x</th>\n",
       "      <th>avg_sentence_distance_y</th>\n",
       "      <th>sentence_x</th>\n",
       "      <th>sentence_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1651691</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There has not been a proclamation about Mr. Mc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1650749</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A 55-year-old woman came forward to the police...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1165597</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was no immediate claim of responsibility...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1322807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The whale stranding was the largest in the cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1598136</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In each newsletter, our gender writer, Maya Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9672</th>\n",
       "      <td>1450768</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zhaira Franco, 35, who works for Facebook in s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>1450768</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An aftershock with a magnitude of 5.7 and an e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9698</th>\n",
       "      <td>1223039</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Schlafly’s obituary will be posted soon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>1223039</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>” A full version of Mrs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>1883246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 27-year-old driver, who failed a breath-al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      entry_id  version_x  version_y  sent_idx_x  sent_idx_y  \\\n",
       "24     1651691          0          1         NaN         6.0   \n",
       "42     1650749          0          1         NaN         1.0   \n",
       "155    1165597          0          1         NaN         3.0   \n",
       "245    1322807          1          2         NaN         7.0   \n",
       "307    1598136          3          4         NaN         1.0   \n",
       "...        ...        ...        ...         ...         ...   \n",
       "9672   1450768          0          1         NaN         4.0   \n",
       "9676   1450768          0          1         NaN         6.0   \n",
       "9698   1223039          0          1         NaN         7.0   \n",
       "9709   1223039          0          1         NaN         6.0   \n",
       "9731   1883246          0          1         NaN         5.0   \n",
       "\n",
       "      avg_sentence_distance_x  avg_sentence_distance_y sentence_x  \\\n",
       "24                        NaN                      NaN        NaN   \n",
       "42                        NaN                      NaN        NaN   \n",
       "155                       NaN                      NaN        NaN   \n",
       "245                       NaN                      NaN        NaN   \n",
       "307                       NaN                      NaN        NaN   \n",
       "...                       ...                      ...        ...   \n",
       "9672                      NaN                      NaN        NaN   \n",
       "9676                      NaN                      NaN        NaN   \n",
       "9698                      NaN                      NaN        NaN   \n",
       "9709                      NaN                      NaN        NaN   \n",
       "9731                      NaN                      NaN        NaN   \n",
       "\n",
       "                                             sentence_y  \n",
       "24    There has not been a proclamation about Mr. Mc...  \n",
       "42    A 55-year-old woman came forward to the police...  \n",
       "155   There was no immediate claim of responsibility...  \n",
       "245   The whale stranding was the largest in the cou...  \n",
       "307   In each newsletter, our gender writer, Maya Sa...  \n",
       "...                                                 ...  \n",
       "9672  Zhaira Franco, 35, who works for Facebook in s...  \n",
       "9676  An aftershock with a magnitude of 5.7 and an e...  \n",
       "9698           Schlafly’s obituary will be posted soon.  \n",
       "9709                           ” A full version of Mrs.  \n",
       "9731  The 27-year-old driver, who failed a breath-al...  \n",
       "\n",
       "[327 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many additions there are\n",
    "merged_matched_sentences.loc[lambda df: df['sent_idx_x'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

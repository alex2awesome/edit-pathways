{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "import sparknlp.annotator as sa\n",
    "import sparknlp.base as sb\n",
    "import sparknlp\n",
    "from sparknlp import Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from util import util_data_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "util_access.download_file('newssniffer-nytimes.db.gz', 'edit-pathways/dbs/newssniffer-nytimes.db.gz')\n",
    "! gunzip newssniffer-nytimes.db.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark = sparknlp.start()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .config(\"spark.executor.instances\", \"30\")\n",
    "      .config(\"spark.driver.memory\", \"20g\")\n",
    "      .config(\"spark.executor.memory\", \"20g\")\n",
    "      .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "      .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5\")\n",
    "      .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "                <div>\n",
       "                    <p><b>SparkContext</b></p>\n",
       "                    <p><a href=\"/sprk/4040/jobs/\">Spark UI</a></p>\n",
       "                    <dl>\n",
       "                      <dt>Version</dt>\n",
       "                        <dd><code>v2.4.4</code></dd>\n",
       "                      <dt>AppName</dt>\n",
       "                        <dd><code>pyspark-shell</code></dd>\n",
       "                    </dl>\n",
       "                    <br>\n",
       "                    <b>Executor Status</b>\n",
       "                    <dl>\n",
       "                      <dt>Running</dt>\n",
       "                        <dd><code>15</code></dd>\n",
       "                      <dt>Pending</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                      <dt>Failed</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                    </dl>\n",
       "                </div>\n",
       "                \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5f3c6e15f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Our Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pyspark.sql.functions as F\n",
    "# import unidecode\n",
    "\n",
    "# conn = sqlite3.connect('../data/diffengine-diffs/db/newssniffer-nytimes.db')\n",
    "conn = sqlite3.connect('newssniffer-nytimes.db')\n",
    "\n",
    "df = pd.read_sql('''\n",
    "     SELECT * from entryversion \n",
    "     WHERE entry_id IN (SELECT distinct entry_id FROM entryversion LIMIT 2)\n",
    " ''', con=conn)\n",
    "\n",
    "# df = pd.read_sql('''\n",
    "#     SELECT entry_id, summary, version from entryversion \n",
    "# ''', con=conn)\n",
    "\n",
    "df = df.assign(summary=lambda df: df['summary'].str.replace('</p><p>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Sentence Tokenizing on Our Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documenter = sb.DocumentAssembler()\\\n",
    "    .setInputCol(\"summary\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentencer = (sa.SentenceDetector()\n",
    "                .setInputCols([\"document\"])\n",
    "                .setOutputCol(\"sentences\")            \n",
    "            )\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols([\"sentences\"]) \n",
    ")\n",
    "\n",
    "sd_pipeline = PipelineModel(stages=[documenter, sentencer, finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = sd_pipeline.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list_df = (annotations_df\n",
    "                .select(\"entry_id\", \"version\", F.posexplode(\"finished_sentences\"))\n",
    "                .withColumnRenamed('col', 'sentence')\n",
    "                .withColumnRenamed('pos', 'sent_idx')\n",
    "               )\n",
    "# tdf = sent_list_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_sent_df = (sent_list_df\n",
    " .alias(\"sent_list_df\")\n",
    " .join(\n",
    "     sent_list_df.alias(\"sent_list_df_2\"),\n",
    "     [F.col(\"sent_list_df.entry_id\") == F.col(\"sent_list_df_2.entry_id\"), \n",
    "      F.col(\"sent_list_df.version\") == F.col(\"sent_list_df_2.version\"), \n",
    "     ], \n",
    "     \"inner\"\n",
    " )\n",
    " .select(\n",
    "     F.col(\"sent_list_df.entry_id\"),\n",
    "     F.col(\"sent_list_df.version\"),\n",
    "     F.col(\"sent_list_df.sent_idx\").alias(\"sent_idx_x\"),\n",
    "     F.col(\"sent_list_df_2.sent_idx\").alias(\"sent_idx_y\"),\n",
    "     F.col(\"sent_list_df.sentence\").alias(\"sentence_x\"),\n",
    "     F.col(\"sent_list_df_2.sentence\").alias(\"sentence_y\"),\n",
    "#    .show(truncate=False)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+-------------------+--------------------+\n",
      "|entry_id|version|sent_idx_x|sent_idx_y|         sentence_x|          sentence_y|\n",
      "+--------+-------+----------+----------+-------------------+--------------------+\n",
      "|  548743|      1|         0|         0|FORT COLLINS, Colo.| FORT COLLINS, Colo.|\n",
      "|  548743|      1|         0|         1|FORT COLLINS, Colo.|— Annie Hartnett ...|\n",
      "|  548743|      1|         0|         2|FORT COLLINS, Colo.|Now 21 and a lead...|\n",
      "|  548743|      1|         0|         3|FORT COLLINS, Colo.|“I would still sa...|\n",
      "|  548743|      1|         0|         4|FORT COLLINS, Colo.|“When you’re voti...|\n",
      "|  548743|      1|         0|         5|FORT COLLINS, Colo.|” So on Saturday ...|\n",
      "|  548743|      1|         0|         6|FORT COLLINS, Colo.|Each party used t...|\n",
      "|  548743|      1|         0|         7|FORT COLLINS, Colo.|But Mr. Obama, tr...|\n",
      "|  548743|      1|         0|         8|FORT COLLINS, Colo.|“I’m counting on ...|\n",
      "|  548743|      1|         0|         9|FORT COLLINS, Colo.|“Those who oppose...|\n",
      "|  548743|      1|         0|        10|FORT COLLINS, Colo.|But throughout Am...|\n",
      "|  548743|      1|         0|        11|FORT COLLINS, Colo.|And they’re going...|\n",
      "|  548743|      1|         0|        12|FORT COLLINS, Colo.|” “I’m asking you...|\n",
      "|  548743|      1|         0|        13|FORT COLLINS, Colo.|From Iowa State, ...|\n",
      "|  548743|      1|         0|        14|FORT COLLINS, Colo.|That itinerary of...|\n",
      "|  548743|      1|         0|        15|FORT COLLINS, Colo.|“I do think that ...|\n",
      "|  548743|      1|         0|        16|FORT COLLINS, Colo.|“What’s driving t...|\n",
      "|  548743|      1|         0|        17|FORT COLLINS, Colo.|” While Gallup’s ...|\n",
      "|  548743|      1|         0|        18|FORT COLLINS, Colo.|Then, 18 percent ...|\n",
      "|  548743|      1|         0|        19|FORT COLLINS, Colo.|But they voted ov...|\n",
      "+--------+-------+----------+----------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_sent_df.show()\n",
    "\n",
    "\n",
    "## todo: \n",
    "## 0. do this same procedure for diffed sequential versions\n",
    "\n",
    "## 1a. use tokenize and Albert or BERT or Word2Vec to generate vectors of embeddings for each sentence.\n",
    "## 1b. lemmatize each sentence\n",
    "\n",
    "## 2. take Sim_asym along each row, two times using:\n",
    "## a. phi(x, y) = vec(x) \\cdot vec(y)\n",
    "## b. phi(x ,y) = lemmatization\n",
    "\n",
    "## 3. for each sentence, select the argmax in both directions.\n",
    "## 4. choose some reasonable threshold.\n",
    "\n",
    "## 5. For scores above this threshold, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000\n",
    "unique_entryids = df['entry_id'].unique()\n",
    "num_chunks = int(unique_entryids.shape[0] / chunksize)\n",
    "\n",
    "output_dfs = []\n",
    "for chunk_id in tqdm(range(num_chunks)):\n",
    "    batch_ids = unique_entryids[chunk_id * chunksize: (chunk_id + 1) * chunksize]\n",
    "    small_df = df.loc[lambda df: df['entry_id'].isin(batch_ids)]\n",
    "    #\n",
    "    sdf = spark.createDataFrame(small_df)\n",
    "    #\n",
    "    annotations_df = sd_pipeline.transform(sdf)\n",
    "    t_df = annotations_df.toPandas()\n",
    "    output_dfs.append(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Albert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = (\n",
    "      sb.DocumentAssembler()\n",
    "        .setInputCol(\"summary\")\n",
    "        .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    sa.Tokenizer()\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"token\")\n",
    ")\n",
    " \n",
    "word_embeddings = (\n",
    "    sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_xxlarge_uncased_en')\n",
    "        .setInputCols([\"document\", \"token\"])\n",
    "        .setOutputCol(\"embeddings\")\n",
    ")\n",
    "\n",
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "            .setInputCols(\"embeddings\")\n",
    "            .setOutputCols(\"embeddings_vectors\")\n",
    "            .setOutputAsVector(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pipeline = Pipeline(stages=\n",
    "  [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    embeddings_finisher\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert = bert_pipeline.fit(sdf).transform(sdf)\n",
    "# df_bert = bert_pipeline_model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, version: bigint, title: string, created: string, url: string, source: string, entry_id: bigint, archive_url: string, num_versions: bigint, summary: string, joint_key: string, id: string, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, token: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, embeddings: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, embeddings_vectors: array<vector>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert#.select('entry_id', 'version', 'embedding_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_df = (df_bert\n",
    "         .select('entry_id', 'version', 'embeddings_vectors')\n",
    "         .toPandas()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer, SQLTransformer\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "documenter = (\n",
    "    sb.DocumentAssembler()\n",
    "        .setInputCol(\"summary\")\n",
    "        .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "sentencer = (\n",
    "    sa.SentenceDetector()\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"sentences\")            \n",
    ")\n",
    "\n",
    "explode_sentences = (\n",
    "    SQLTransformer()\n",
    "     .setStatement(\"SELECT entry_id, version, POSEXPLODE(sentences) AS (sent_idx, sentence), * FROM __THIS__\")\n",
    ")\n",
    "\n",
    "documenter = (\n",
    "    sb.DocumentAssembler()\n",
    "        .setInputCol(\"summary\")\n",
    "        .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    sa.Tokenizer()\n",
    "        .setInputCols([\"sentence\"])\n",
    "        .setOutputCol(\"token\")\n",
    ")\n",
    " \n",
    "word_embeddings = (\n",
    "    sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_large_uncased_en')\n",
    "        .setInputCols([\"sentence\", \"token\"])\n",
    "        .setOutputCol(\"embeddings\")\n",
    ")\n",
    "\n",
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "            .setInputCols(\"embeddings\")\n",
    "            .setOutputCols(\"embeddings_vectors\")\n",
    "            .setOutputAsVector(True)\n",
    ")\n",
    "\n",
    "explode_word_embeddings = (\n",
    "    SQLTransformer()\n",
    "     .setStatement(\"SELECT entry_id, version, sent_idx, POSEXPLODE(embeddings_vectors) AS (word_idx, word_embedding), * FROM __THIS__\")\n",
    ")\n",
    "\n",
    "vector_normalizer = (\n",
    "    Normalizer()\n",
    "    .setInputCol(\"word_embedding\")\n",
    "    .setOutputCol(\"norm_word_embedding\")\n",
    "    .setP(1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=\n",
    "  [\n",
    "    documenter,\n",
    "    sentencer,\n",
    "    explode_sentences,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    embeddings_finisher,\n",
    "    explode_word_embeddings,\n",
    "    vector_normalizer\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "\"requirement failed: Wrong or missing inputCols annotators in REGEX_TOKENIZER_38db707132ad.\\n\\nCurrent inputCols: sentence. Dataset's columns:\\n(column_name=entry_id,is_nlp_annotator=false)\\n(column_name=version,is_nlp_annotator=false)\\n(column_name=sent_idx,is_nlp_annotator=false)\\n(column_name=sentence,is_nlp_annotator=false)\\n(column_name=index,is_nlp_annotator=false)\\n(column_name=version,is_nlp_annotator=false)\\n(column_name=title,is_nlp_annotator=false)\\n(column_name=created,is_nlp_annotator=false)\\n(column_name=url,is_nlp_annotator=false)\\n(column_name=source,is_nlp_annotator=false)\\n(column_name=entry_id,is_nlp_annotator=false)\\n(column_name=archive_url,is_nlp_annotator=false)\\n(column_name=num_versions,is_nlp_annotator=false)\\n(column_name=summary,is_nlp_annotator=false)\\n(column_name=joint_key,is_nlp_annotator=false)\\n(column_name=id,is_nlp_annotator=false)\\n(column_name=document,is_nlp_annotator=true,type=document)\\n(column_name=sentences,is_nlp_annotator=true,type=document).\\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o798.transform.\n: java.lang.IllegalArgumentException: requirement failed: Wrong or missing inputCols annotators in REGEX_TOKENIZER_38db707132ad.\n\nCurrent inputCols: sentence. Dataset's columns:\n(column_name=entry_id,is_nlp_annotator=false)\n(column_name=version,is_nlp_annotator=false)\n(column_name=sent_idx,is_nlp_annotator=false)\n(column_name=sentence,is_nlp_annotator=false)\n(column_name=index,is_nlp_annotator=false)\n(column_name=version,is_nlp_annotator=false)\n(column_name=title,is_nlp_annotator=false)\n(column_name=created,is_nlp_annotator=false)\n(column_name=url,is_nlp_annotator=false)\n(column_name=source,is_nlp_annotator=false)\n(column_name=entry_id,is_nlp_annotator=false)\n(column_name=archive_url,is_nlp_annotator=false)\n(column_name=num_versions,is_nlp_annotator=false)\n(column_name=summary,is_nlp_annotator=false)\n(column_name=joint_key,is_nlp_annotator=false)\n(column_name=id,is_nlp_annotator=false)\n(column_name=document,is_nlp_annotator=true,type=document)\n(column_name=sentences,is_nlp_annotator=true,type=document).\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.johnsnowlabs.nlp.AnnotatorModel._transform(AnnotatorModel.scala:43)\n\tat com.johnsnowlabs.nlp.AnnotatorModel.transform(AnnotatorModel.scala:79)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3f22ed00e4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: \"requirement failed: Wrong or missing inputCols annotators in REGEX_TOKENIZER_38db707132ad.\\n\\nCurrent inputCols: sentence. Dataset's columns:\\n(column_name=entry_id,is_nlp_annotator=false)\\n(column_name=version,is_nlp_annotator=false)\\n(column_name=sent_idx,is_nlp_annotator=false)\\n(column_name=sentence,is_nlp_annotator=false)\\n(column_name=index,is_nlp_annotator=false)\\n(column_name=version,is_nlp_annotator=false)\\n(column_name=title,is_nlp_annotator=false)\\n(column_name=created,is_nlp_annotator=false)\\n(column_name=url,is_nlp_annotator=false)\\n(column_name=source,is_nlp_annotator=false)\\n(column_name=entry_id,is_nlp_annotator=false)\\n(column_name=archive_url,is_nlp_annotator=false)\\n(column_name=num_versions,is_nlp_annotator=false)\\n(column_name=summary,is_nlp_annotator=false)\\n(column_name=joint_key,is_nlp_annotator=false)\\n(column_name=id,is_nlp_annotator=false)\\n(column_name=document,is_nlp_annotator=true,type=document)\\n(column_name=sentences,is_nlp_annotator=true,type=document).\\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document\""
     ]
    }
   ],
   "source": [
    "sent_sdf = pipeline.fit(sdf).transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sdf = bert_pipeline.fit(sent_list_df).transform(sent_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_emb_sdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-58eeee5088cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msnet_list_dfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_emb_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_emb_sdf' is not defined"
     ]
    }
   ],
   "source": [
    "snet_list_dfp = word_emb_sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_word_df = (word_emb_sdf\n",
    " .alias(\"word_emb_df\")\n",
    " .join(\n",
    "     sent_list_df.alias(\"word_emb_df_2\"),\n",
    "     [F.col(\"word_emb_df.entry_id\") == F.col(\"word_emb_df.entry_id\"), \n",
    "      F.col(\"word_emb_df.version\") == F.col(\"word_emb_df.version\"), \n",
    "     ], \n",
    "     \"inner\"\n",
    " )\n",
    " .select(\n",
    "     F.col(\"word_emb_df.entry_id\"),\n",
    "     F.col(\"word_emb_df.version\"),\n",
    "     # sent_idx \n",
    "     F.col(\"word_emb_df.sent_idx\").alias(\"sent_idx_x\"),\n",
    "     F.col(\"word_emb_df_2.sent_idx\").alias(\"sent_idx_y\"),\n",
    "     # word_idx\n",
    "     F.col(\"word_emb_df.word_idx\").alias(\"word_idx_x\"),\n",
    "     F.col(\"word_emb_df_2.word_idx\").alias(\"word_idx_y\"),\n",
    "     # word_emb\n",
    "     F.col(\"word_emb_df.word_embedding\").alias(\"word_embedding_x\"),\n",
    "     F.col(\"word_emb_df_2.word_embedding\").alias(\"word_embedding_y\"),\n",
    "#    .show(truncate=False)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------------+--------------------+--------------------+-------+--------+--------------------+------------+--------------------+---------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|version|               title|             created|                 url| source|entry_id|         archive_url|num_versions|             summary|joint_key|      id|            document|           sentences|               token|          embeddings|  embeddings_vectors|\n",
      "+-----+-------+--------------------+--------------------+--------------------+-------+--------+--------------------+------------+--------------------+---------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|68763|      0|Activist Challeng...|2012-08-26 22:55:...|http://www.nytime...|nytimes|  547988|https://www.newss...|           2|In Silicon Valley...| 547988-0|547988-0|[[document, 0, 84...|[[document, 0, 15...|[[token, 0, 1, In...|[[word_embeddings...|[[-0.893178820610...|\n",
      "|68764|      1|Inventor Challeng...|2012-08-27 02:55:...|http://www.nytime...|nytimes|  547988|https://www.newss...|           2|In Silicon Valley...| 547988-1|547988-1|[[document, 0, 84...|[[document, 0, 15...|[[token, 0, 1, In...|[[word_embeddings...|[[-0.893178820610...|\n",
      "|68765|      0|U.S. Foreign Arms...|2012-08-26 22:55:...|http://www.nytime...|nytimes|  547989|https://www.newss...|           3|WASHINGTON — Weap...| 547989-0|547989-0|[[document, 0, 34...|[[document, 0, 21...|[[token, 0, 9, WA...|[[word_embeddings...|[[0.6029991507530...|\n",
      "|68766|      1|U.S. Foreign Arms...|2012-08-27 00:10:...|http://www.nytime...|nytimes|  547989|https://www.newss...|           3|WASHINGTON — Weap...| 547989-1|547989-1|[[document, 0, 34...|[[document, 0, 21...|[[token, 0, 9, WA...|[[word_embeddings...|[[0.6029991507530...|\n",
      "|68767|      2|U.S. Arms Sales M...|2012-08-27 01:20:...|http://www.nytime...|nytimes|  547989|https://www.newss...|           3|WASHINGTON — Weap...| 547989-2|547989-2|[[document, 0, 34...|[[document, 0, 21...|[[token, 0, 9, WA...|[[word_embeddings...|[[0.6029991507530...|\n",
      "+-----+-------+--------------------+--------------------+--------------------+-------+--------+--------------------+------------+--------------------+---------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bert.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df_bert.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp['sentneces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>embeddings_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(token, 0, 1, In, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[[-0.8931788206100464, -0.3664441406726837, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(token, 0, 1, In, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[[-0.8931788206100464, -0.3664441406726837, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(token, 0, 9, WASHINGTON, {'sentence': '0'}, ...</td>\n",
       "      <td>[[0.6029991507530212, -0.002772439271211624, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(token, 0, 9, WASHINGTON, {'sentence': '0'}, ...</td>\n",
       "      <td>[[0.6029991507530212, -0.002772439271211624, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(token, 0, 9, WASHINGTON, {'sentence': '0'}, ...</td>\n",
       "      <td>[[0.6029991507530212, -0.002772439271211624, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token  \\\n",
       "0  [(token, 0, 1, In, {'sentence': '0'}, []), (to...   \n",
       "1  [(token, 0, 1, In, {'sentence': '0'}, []), (to...   \n",
       "2  [(token, 0, 9, WASHINGTON, {'sentence': '0'}, ...   \n",
       "3  [(token, 0, 9, WASHINGTON, {'sentence': '0'}, ...   \n",
       "4  [(token, 0, 9, WASHINGTON, {'sentence': '0'}, ...   \n",
       "\n",
       "                                  embeddings_vectors  \n",
       "0  [[-0.8931788206100464, -0.3664441406726837, -0...  \n",
       "1  [[-0.8931788206100464, -0.3664441406726837, -0...  \n",
       "2  [[0.6029991507530212, -0.002772439271211624, -...  \n",
       "3  [[0.6029991507530212, -0.002772439271211624, -...  \n",
       "4  [[0.6029991507530212, -0.002772439271211624, -...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp[['token', 'embeddings_vectors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(annotatorType='token', begin=0, end=1, result='In', metadata={'sentence': '0'}, embeddings=[])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp['token'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(annotatorType='token', begin=529, end=535, result='himself', metadata={'sentence': '2'}, embeddings=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp['token'][0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1570"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfp['embeddings_vectors'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = dfp['embeddings_vectors'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = (\n",
    "    sa.Tokenizer()\n",
    "        .setInputCols([\"sentences\"])\n",
    "        .setOutputCol(\"token\")\n",
    ")\n",
    "\n",
    "word_embeddings = (\n",
    "    sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_large_uncased_en')\n",
    "        .setInputCols([\"sentences\", \"token\"])\n",
    "        .setOutputCol(\"embeddings\")\n",
    "        .setBatchSize(100)\n",
    ")\n",
    "\n",
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "            .setInputCols(\"embeddings\")\n",
    "            .setOutputCols(\"embeddings_vectors\")\n",
    "            .setOutputAsVector(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "                <div>\n",
       "                    <p><b>SparkContext</b></p>\n",
       "                    <p><a href=\"/sprk/4040/jobs/\">Spark UI</a></p>\n",
       "                    <dl>\n",
       "                      <dt>Version</dt>\n",
       "                        <dd><code>v2.4.4</code></dd>\n",
       "                      <dt>AppName</dt>\n",
       "                        <dd><code>pyspark-shell</code></dd>\n",
       "                    </dl>\n",
       "                    <br>\n",
       "                    <b>Executor Status</b>\n",
       "                    <dl>\n",
       "                      <dt>Running</dt>\n",
       "                        <dd><code>18</code></dd>\n",
       "                      <dt>Pending</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                      <dt>Failed</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                    </dl>\n",
       "                </div>\n",
       "                \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f44023b5978>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_bert\n",
    " .select('entry_id', 'version', 'sentences', 'embeddings_vectors')\n",
    " .write.mode(\"overwrite\").parquet(\"s3://aspangher/tmp/tmp_albert_embeddings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert.select('entry_id', 'version', 'embeddings_vectors').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = bert_pipeline_from_sentences.fit(spark.createDataFrame([[\"\"]]).toDF(\"summary\"))\n",
    "result = pipeline_model.transform(spark.createDataFrame(pd.DataFrame({\"summary\": [\"I love NLP\"]})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_df = (df_bert\n",
    "         .select('entry_id', 'version', 'sent_idx', 'sentence', 'embeddings_vectors')\n",
    "         .toPandas()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = CoNLL().readDataset(spark, 's3://aspangher/spark-nlp/conll/eng.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "training_data = CoNLL().readDataset(spark, 's3://aspangher/spark-nlp/conll/eng.train')\n",
    "\n",
    "get_embeddings = (sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_large_uncased_en')\n",
    "        .setInputCols(\"sentence\", \"token\")\n",
    "        .setOutputCol(\"embeddings\")\n",
    "        .setMaxSentenceLength(100)\n",
    "        .setBatchSize(8)\n",
    ")\n",
    "\n",
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "        .setInputCols(\"embeddings\")\n",
    "        .setOutputCols(\"embeddings_vectors\")\n",
    "        .setOutputAsVector(True)\n",
    ")\n",
    "\n",
    "sentence_finisher = (\n",
    "    Finisher()\n",
    "       .setInputCols([\"sentence\"]) \n",
    ")\n",
    "\n",
    "pipeline =  Pipeline(stages=[\n",
    "    get_embeddings, \n",
    "    embeddings_finisher, \n",
    "    sentence_finisher\n",
    "])\n",
    "\n",
    "pipelineDF = pipeline.fit(training_data).transform(training_data)\n",
    "\n",
    "(pipelineDF\n",
    " .select('finished_sentence', 'embeddings_vectors')\n",
    " .write\n",
    " .mode(\"overwrite\").parquet('s3://aspangher/tmp/tmp_conll_albert_embeddings.pq')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.5'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineDF.select('finished_sentence', 'embeddings_vectors').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = RecursivePipeline(stages=[\n",
    "    DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\"), \n",
    "    SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\"), \n",
    "    Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\").setMaxLength(100).setSplitChars([\"-\", \"\\xa0\", \"—\"]), \n",
    "    BertEmbeddings.pretrained(name = \"bert_large_cased\", lang='en').setInputCols(['sentence', 'token']).setOutputCol('embeddings'), \n",
    "#     NerDLModel.pretrained('onto_bert_large_cased', 'en').setInputCols(['sentence', 'token', 'embeddings']).setOutputCol('ner'), \n",
    "#     NerConverter().setInputCols(['sentence', 'token', 'ner']).setOutputCol('ner_chunk') \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

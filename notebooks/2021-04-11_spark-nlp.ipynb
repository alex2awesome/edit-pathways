{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "import sparknlp.annotator as sa\n",
    "import sparknlp.base as sb\n",
    "import sparknlp\n",
    "from sparknlp import Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark = sparknlp.start()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .config(\"spark.executor.instances\", \"30\")\n",
    "      .config(\"spark.driver.memory\", \"20g\")\n",
    "      .config(\"spark.executor.memory\", \"20g\")\n",
    "      .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "      .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5\")\n",
    "      .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "                <div>\n",
       "                    <p><b>SparkContext</b></p>\n",
       "                    <p><a href=\"/sprk/4040/jobs/\">Spark UI</a></p>\n",
       "                    <dl>\n",
       "                      <dt>Version</dt>\n",
       "                        <dd><code>v2.4.4</code></dd>\n",
       "                      <dt>AppName</dt>\n",
       "                        <dd><code>pyspark-shell</code></dd>\n",
       "                    </dl>\n",
       "                    <br>\n",
       "                    <b>Executor Status</b>\n",
       "                    <dl>\n",
       "                      <dt>Running</dt>\n",
       "                        <dd><code>20</code></dd>\n",
       "                      <dt>Pending</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                      <dt>Failed</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                    </dl>\n",
       "                </div>\n",
       "                \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe309730390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Our Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pyspark.sql.functions as F\n",
    "# import unidecode\n",
    "\n",
    "# conn = sqlite3.connect('../data/diffengine-diffs/db/newssniffer-nytimes.db')\n",
    "conn = sqlite3.connect('newssniffer-nytimes.db')\n",
    "\n",
    "df = pd.read_sql('''\n",
    "     SELECT * from entryversion \n",
    "     WHERE entry_id IN (SELECT distinct entry_id FROM entryversion LIMIT 200)\n",
    " ''', con=conn)\n",
    "\n",
    "# df = pd.read_sql('''\n",
    "#     SELECT entry_id, summary, version from entryversion \n",
    "# ''', con=conn)\n",
    "\n",
    "df = df.assign(summary=lambda df: df['summary'].str.replace('</p><p>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Sentence Tokenizing on Our Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documenter = sb.DocumentAssembler()\\\n",
    "    .setInputCol(\"summary\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentencer = (sa.SentenceDetector()\n",
    "                .setInputCols([\"document\"])\n",
    "                .setOutputCol(\"sentences\")            \n",
    "            )\n",
    "\n",
    "finisher = (\n",
    "    Finisher()\n",
    "    .setInputCols([\"sentences\"]) \n",
    ")\n",
    "\n",
    "sd_pipeline = PipelineModel(stages=[documenter, sentencer, finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = sd_pipeline.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list_df = (annotations_df\n",
    "                .select(\"entry_id\", \"version\", F.posexplode(\"finished_sentences\"))\n",
    "                .withColumnRenamed('col', 'sentence')\n",
    "                .withColumnRenamed('pos', 'sent_idx')\n",
    "               )\n",
    "# tdf = sent_list_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_sent_df = (sent_list_df\n",
    " .alias(\"sent_list_df\")\n",
    " .join(\n",
    "     sent_list_df.alias(\"sent_list_df_2\"),\n",
    "     [F.col(\"sent_list_df.entry_id\") == F.col(\"sent_list_df_2.entry_id\"), \n",
    "      F.col(\"sent_list_df.version\") == F.col(\"sent_list_df_2.version\"), \n",
    "     ], \n",
    "     \"inner\"\n",
    " )\n",
    " .select(\n",
    "     F.col(\"sent_list_df.entry_id\"),\n",
    "     F.col(\"sent_list_df.version\"),\n",
    "     F.col(\"sent_list_df.sent_idx\").alias(\"sent_idx_x\"),\n",
    "     F.col(\"sent_list_df_2.sent_idx\").alias(\"sent_idx_y\"),\n",
    "     F.col(\"sent_list_df.sentence\").alias(\"sentence_x\"),\n",
    "     F.col(\"sent_list_df_2.sentence\").alias(\"sentence_y\"),\n",
    "#    .show(truncate=False)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+-------------------+--------------------+\n",
      "|entry_id|version|sent_idx_x|sent_idx_y|         sentence_x|          sentence_y|\n",
      "+--------+-------+----------+----------+-------------------+--------------------+\n",
      "|  548743|      1|         0|         0|FORT COLLINS, Colo.| FORT COLLINS, Colo.|\n",
      "|  548743|      1|         0|         1|FORT COLLINS, Colo.|— Annie Hartnett ...|\n",
      "|  548743|      1|         0|         2|FORT COLLINS, Colo.|Now 21 and a lead...|\n",
      "|  548743|      1|         0|         3|FORT COLLINS, Colo.|“I would still sa...|\n",
      "|  548743|      1|         0|         4|FORT COLLINS, Colo.|“When you’re voti...|\n",
      "|  548743|      1|         0|         5|FORT COLLINS, Colo.|” So on Saturday ...|\n",
      "|  548743|      1|         0|         6|FORT COLLINS, Colo.|Each party used t...|\n",
      "|  548743|      1|         0|         7|FORT COLLINS, Colo.|But Mr. Obama, tr...|\n",
      "|  548743|      1|         0|         8|FORT COLLINS, Colo.|“I’m counting on ...|\n",
      "|  548743|      1|         0|         9|FORT COLLINS, Colo.|“Those who oppose...|\n",
      "|  548743|      1|         0|        10|FORT COLLINS, Colo.|But throughout Am...|\n",
      "|  548743|      1|         0|        11|FORT COLLINS, Colo.|And they’re going...|\n",
      "|  548743|      1|         0|        12|FORT COLLINS, Colo.|” “I’m asking you...|\n",
      "|  548743|      1|         0|        13|FORT COLLINS, Colo.|From Iowa State, ...|\n",
      "|  548743|      1|         0|        14|FORT COLLINS, Colo.|That itinerary of...|\n",
      "|  548743|      1|         0|        15|FORT COLLINS, Colo.|“I do think that ...|\n",
      "|  548743|      1|         0|        16|FORT COLLINS, Colo.|“What’s driving t...|\n",
      "|  548743|      1|         0|        17|FORT COLLINS, Colo.|” While Gallup’s ...|\n",
      "|  548743|      1|         0|        18|FORT COLLINS, Colo.|Then, 18 percent ...|\n",
      "|  548743|      1|         0|        19|FORT COLLINS, Colo.|But they voted ov...|\n",
      "+--------+-------+----------+----------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_sent_df.show()\n",
    "\n",
    "\n",
    "## todo: \n",
    "## 0. do this same procedure for diffed sequential versions\n",
    "\n",
    "## 1a. use tokenize and Albert or BERT or Word2Vec to generate vectors of embeddings for each sentence.\n",
    "## 1b. lemmatize each sentence\n",
    "\n",
    "## 2. take Sim_asym along each row, two times using:\n",
    "## a. phi(x, y) = vec(x) \\cdot vec(y)\n",
    "## b. phi(x ,y) = lemmatization\n",
    "\n",
    "## 3. for each sentence, select the argmax in both directions.\n",
    "## 4. choose some reasonable threshold.\n",
    "\n",
    "## 5. For scores above this threshold, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>version</th>\n",
       "      <th>pos</th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In Silicon Valley, Apple just won big against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Across the country, in a federal court in Flor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Mr. Stadnyk, who holds a patent on a motorcycl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Represented by a prominent Washington lawyer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Mr. Stadnyk and his lawyer — along with some a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>The present system, one of the nation’s oldest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>The impending law would overturn that by award...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Mr. Stadnyk, 48, a garage inventor who stumble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>He devised a system of brackets and gears to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>With his system, he says, the rider feels a fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Mr. Stadnyk, who describes his invention style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Today, MadStad employs eight people, including...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>His adjustable windshield systems, priced from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>Yearly sales, he said, are more than $500,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>Mr. Stadnyk holds three patents, and he speaks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>“It came out of your mind,” he explained.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>“It’s not property you bought or inherited.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>” Mr. Stadnyk became interested in the patent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>He says he studied the proposals and the law, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>A grass-roots activist, he even made a couple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>The shift to a first-to-file system, scheduled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>The switch would also put the United States in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>The efficiency argument is the mainstream view...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>President Obama declared that the new law woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>” But opponents say it will give big companies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>Large corporations have deep pockets and armie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>Yet the opponents are in the minority.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>And there is genuine debate about how much gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>Mr. Stadnyk sees his courtroom quest as the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>547988</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>“Others took on Obamacare, and this is my figh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33760</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Faced with public dissatisfaction over traffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33761</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>The local government initiatives are not the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33762</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>The government clamped down on credit a year a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33763</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>Other broad economic problems have been buildi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33764</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>These include industrial overcapacity and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33765</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>But for now, the growing regulatory burden on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33766</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>“That’s why I think the slowdown is likely to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33767</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>Polluting factories being pushed out of increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33768</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>“There is no hint that these costs will be low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33769</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>Some executives in China complain about rising...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33770</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>Critics in the business community say that an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33771</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>But while the local measures may limit short-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33772</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>China is no longer just a developing economy t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33773</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>It is becoming a modern, industrialized econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33774</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>The question is how much short-term pain will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33775</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>Ma Jun, the director of the Institute of Publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33776</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>In each case, local officials agreed to halt c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33777</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>Bernadette Brennan, a senior lawyer in the Bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33778</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>Instead of resisting pressure to address pollu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33779</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>Measuring the environmental benefits of the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33780</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>A series of typhoons makes it hard to compare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33781</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>In Guangzhou, emissions of a wide range of pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33782</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>Emissions started to rise in 2011 as growth re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33783</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>Pollution per dollar of economic output has cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33784</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>Emissions of sulfur dioxide, a top priority in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33785</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>The financial dependence of local governments ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33786</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>But city governments also own many of the busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33787</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>“The car companies are owned by the government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33788</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>“The car companies must obey the government.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33789</th>\n",
       "      <td>551195</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>” He added, “What do we need gross domestic pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33790 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       entry_id  version  pos  \\\n",
       "0        547988        0    0   \n",
       "1        547988        0    1   \n",
       "2        547988        0    2   \n",
       "3        547988        0    3   \n",
       "4        547988        0    4   \n",
       "5        547988        0    5   \n",
       "6        547988        0    6   \n",
       "7        547988        0    7   \n",
       "8        547988        0    8   \n",
       "9        547988        0    9   \n",
       "10       547988        0   10   \n",
       "11       547988        0   11   \n",
       "12       547988        0   12   \n",
       "13       547988        0   13   \n",
       "14       547988        0   14   \n",
       "15       547988        0   15   \n",
       "16       547988        0   16   \n",
       "17       547988        0   17   \n",
       "18       547988        0   18   \n",
       "19       547988        0   19   \n",
       "20       547988        0   20   \n",
       "21       547988        0   21   \n",
       "22       547988        0   22   \n",
       "23       547988        0   23   \n",
       "24       547988        0   24   \n",
       "25       547988        0   25   \n",
       "26       547988        0   26   \n",
       "27       547988        0   27   \n",
       "28       547988        0   28   \n",
       "29       547988        0   29   \n",
       "...         ...      ...  ...   \n",
       "33760    551195        2   14   \n",
       "33761    551195        2   15   \n",
       "33762    551195        2   16   \n",
       "33763    551195        2   17   \n",
       "33764    551195        2   18   \n",
       "33765    551195        2   19   \n",
       "33766    551195        2   20   \n",
       "33767    551195        2   21   \n",
       "33768    551195        2   22   \n",
       "33769    551195        2   23   \n",
       "33770    551195        2   24   \n",
       "33771    551195        2   25   \n",
       "33772    551195        2   26   \n",
       "33773    551195        2   27   \n",
       "33774    551195        2   28   \n",
       "33775    551195        2   29   \n",
       "33776    551195        2   30   \n",
       "33777    551195        2   31   \n",
       "33778    551195        2   32   \n",
       "33779    551195        2   33   \n",
       "33780    551195        2   34   \n",
       "33781    551195        2   35   \n",
       "33782    551195        2   36   \n",
       "33783    551195        2   37   \n",
       "33784    551195        2   38   \n",
       "33785    551195        2   39   \n",
       "33786    551195        2   40   \n",
       "33787    551195        2   41   \n",
       "33788    551195        2   42   \n",
       "33789    551195        2   43   \n",
       "\n",
       "                                                     col  \n",
       "0      In Silicon Valley, Apple just won big against ...  \n",
       "1      Across the country, in a federal court in Flor...  \n",
       "2      Mr. Stadnyk, who holds a patent on a motorcycl...  \n",
       "3      Represented by a prominent Washington lawyer, ...  \n",
       "4      Mr. Stadnyk and his lawyer — along with some a...  \n",
       "5      The present system, one of the nation’s oldest...  \n",
       "6      The impending law would overturn that by award...  \n",
       "7      Mr. Stadnyk, 48, a garage inventor who stumble...  \n",
       "8      He devised a system of brackets and gears to a...  \n",
       "9      With his system, he says, the rider feels a fl...  \n",
       "10     Mr. Stadnyk, who describes his invention style...  \n",
       "11     Today, MadStad employs eight people, including...  \n",
       "12     His adjustable windshield systems, priced from...  \n",
       "13     Yearly sales, he said, are more than $500,000 ...  \n",
       "14     Mr. Stadnyk holds three patents, and he speaks...  \n",
       "15             “It came out of your mind,” he explained.  \n",
       "16           “It’s not property you bought or inherited.  \n",
       "17     ” Mr. Stadnyk became interested in the patent ...  \n",
       "18     He says he studied the proposals and the law, ...  \n",
       "19     A grass-roots activist, he even made a couple ...  \n",
       "20     The shift to a first-to-file system, scheduled...  \n",
       "21     The switch would also put the United States in...  \n",
       "22     The efficiency argument is the mainstream view...  \n",
       "23     President Obama declared that the new law woul...  \n",
       "24     ” But opponents say it will give big companies...  \n",
       "25     Large corporations have deep pockets and armie...  \n",
       "26                Yet the opponents are in the minority.  \n",
       "27     And there is genuine debate about how much gar...  \n",
       "28     Mr. Stadnyk sees his courtroom quest as the in...  \n",
       "29     “Others took on Obamacare, and this is my figh...  \n",
       "...                                                  ...  \n",
       "33760  Faced with public dissatisfaction over traffic...  \n",
       "33761  The local government initiatives are not the m...  \n",
       "33762  The government clamped down on credit a year a...  \n",
       "33763  Other broad economic problems have been buildi...  \n",
       "33764  These include industrial overcapacity and the ...  \n",
       "33765  But for now, the growing regulatory burden on ...  \n",
       "33766  “That’s why I think the slowdown is likely to ...  \n",
       "33767  Polluting factories being pushed out of increa...  \n",
       "33768  “There is no hint that these costs will be low...  \n",
       "33769  Some executives in China complain about rising...  \n",
       "33770  Critics in the business community say that an ...  \n",
       "33771  But while the local measures may limit short-t...  \n",
       "33772  China is no longer just a developing economy t...  \n",
       "33773  It is becoming a modern, industrialized econom...  \n",
       "33774  The question is how much short-term pain will ...  \n",
       "33775  Ma Jun, the director of the Institute of Publi...  \n",
       "33776  In each case, local officials agreed to halt c...  \n",
       "33777  Bernadette Brennan, a senior lawyer in the Bei...  \n",
       "33778  Instead of resisting pressure to address pollu...  \n",
       "33779  Measuring the environmental benefits of the ch...  \n",
       "33780  A series of typhoons makes it hard to compare ...  \n",
       "33781  In Guangzhou, emissions of a wide range of pol...  \n",
       "33782  Emissions started to rise in 2011 as growth re...  \n",
       "33783  Pollution per dollar of economic output has cl...  \n",
       "33784  Emissions of sulfur dioxide, a top priority in...  \n",
       "33785  The financial dependence of local governments ...  \n",
       "33786  But city governments also own many of the busi...  \n",
       "33787  “The car companies are owned by the government...  \n",
       "33788       “The car companies must obey the government.  \n",
       "33789  ” He added, “What do we need gross domestic pr...  \n",
       "\n",
       "[33790 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000\n",
    "unique_entryids = df['entry_id'].unique()\n",
    "num_chunks = int(unique_entryids.shape[0] / chunksize)\n",
    "\n",
    "output_dfs = []\n",
    "for chunk_id in tqdm(range(num_chunks)):\n",
    "    batch_ids = unique_entryids[chunk_id * chunksize: (chunk_id + 1) * chunksize]\n",
    "    small_df = df.loc[lambda df: df['entry_id'].isin(batch_ids)]\n",
    "    #\n",
    "    sdf = spark.createDataFrame(small_df)\n",
    "    #\n",
    "    annotations_df = sd_pipeline.transform(sdf)\n",
    "    t_df = annotations_df.toPandas()\n",
    "    output_dfs.append(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Albert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = (\n",
    "      sb.DocumentAssembler()\n",
    "        .setInputCol(\"summary\")\n",
    "        .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    sa.Tokenizer()\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"token\")\n",
    ")\n",
    " \n",
    "word_embeddings = (\n",
    "    sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_xxlarge_uncased_en')\n",
    "        .setInputCols([\"document\", \"token\"])\n",
    "        .setOutputCol(\"embeddings\")\n",
    ")\n",
    "\n",
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "            .setInputCols(\"embeddings\")\n",
    "            .setOutputCols(\"embeddings_vectors\")\n",
    "            .setOutputAsVector(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = (\n",
    "    sa.Tokenizer()\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"token\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pipeline = Pipeline(stages=\n",
    "  [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    embeddings_finisher\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert = bert_pipeline.fit(sdf).transform(sdf)\n",
    "# df_bert = bert_pipeline_model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, version: bigint, title: string, created: string, url: string, source: string, entry_id: bigint, archive_url: string, num_versions: bigint, summary: string, joint_key: string, id: string, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, token: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, embeddings: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, embeddings_vectors: array<vector>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert#.select('entry_id', 'version', 'embedding_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_df = (df_bert\n",
    "         .select('entry_id', 'version', 'embeddings_vectors')\n",
    "         .toPandas()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documenter = (\n",
    "    sb.DocumentAssembler()\n",
    "        .setInputCol(\"summary\")\n",
    "        .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "sentencer = (\n",
    "    sa.SentenceDetector()\n",
    "        .setInputCols([\"document\"])\n",
    "        .setOutputCol(\"sentences\")            \n",
    ")\n",
    "\n",
    "tokenizer = (\n",
    "    sa.Tokenizer()\n",
    "        .setInputCols([\"sentences\"])\n",
    "        .setOutputCol(\"token\")\n",
    ")\n",
    "\n",
    "word_embeddings = (\n",
    "    sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_large_uncased_en')\n",
    "        .setInputCols([\"sentences\", \"token\"])\n",
    "        .setOutputCol(\"embeddings\")\n",
    "        .setBatchSize(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "            .setInputCols(\"embeddings\")\n",
    "            .setOutputCols(\"embeddings_vectors\")\n",
    "            .setOutputAsVector(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pipeline_from_sentences = Pipeline(stages=\n",
    "  [\n",
    "      documenter,\n",
    "#       sentence_documenter,\n",
    "      sentencer,\n",
    "      tokenizer,\n",
    "      word_embeddings,\n",
    "      embeddings_finisher\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bert = bert_pipeline_from_sentences.fit(sent_list_df).transform(sent_list_df)\n",
    "df_bert = bert_pipeline_from_sentences.fit(sdf).transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bert.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "                <div>\n",
       "                    <p><b>SparkContext</b></p>\n",
       "                    <p><a href=\"/sprk/4040/jobs/\">Spark UI</a></p>\n",
       "                    <dl>\n",
       "                      <dt>Version</dt>\n",
       "                        <dd><code>v2.4.4</code></dd>\n",
       "                      <dt>AppName</dt>\n",
       "                        <dd><code>pyspark-shell</code></dd>\n",
       "                    </dl>\n",
       "                    <br>\n",
       "                    <b>Executor Status</b>\n",
       "                    <dl>\n",
       "                      <dt>Running</dt>\n",
       "                        <dd><code>22</code></dd>\n",
       "                      <dt>Pending</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                      <dt>Failed</dt>\n",
       "                        <dd><code>0</code></dd>\n",
       "                    </dl>\n",
       "                </div>\n",
       "                \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe309730390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o617.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 5.0 failed 4 times, most recent failure: Lost task 5.3 in stage 5.0 (TID 41, 10.178.188.122, executor 5): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.util.NoSuchElementException: next on empty iterator\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n\tat scala.collection.IterableLike$class.head(IterableLike.scala:107)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:154)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:154)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:152)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:152)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:138)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert.calculateEmbeddings(TensorflowAlbert.scala:138)\n\tat com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings.annotate(AlbertEmbeddings.scala:139)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:35)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:34)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.util.NoSuchElementException: next on empty iterator\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n\tat scala.collection.IterableLike$class.head(IterableLike.scala:107)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:154)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:154)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:152)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:152)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:138)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert.calculateEmbeddings(TensorflowAlbert.scala:138)\n\tat com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings.annotate(AlbertEmbeddings.scala:139)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:35)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:34)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9dabdb20299c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m (df_bert\n\u001b[1;32m      2\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'entry_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embeddings_vectors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m  .write.mode(\"overwrite\").parquet(\"s3://aspangher/tmp/tmp_albert_embeddings\"))\n\u001b[0m",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/python3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o617.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 5.0 failed 4 times, most recent failure: Lost task 5.3 in stage 5.0 (TID 41, 10.178.188.122, executor 5): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.util.NoSuchElementException: next on empty iterator\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n\tat scala.collection.IterableLike$class.head(IterableLike.scala:107)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:154)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:154)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:152)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:152)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:138)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert.calculateEmbeddings(TensorflowAlbert.scala:138)\n\tat com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings.annotate(AlbertEmbeddings.scala:139)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:35)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:34)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 33 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.util.NoSuchElementException: next on empty iterator\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n\tat scala.collection.IterableLike$class.head(IterableLike.scala:107)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7$$anonfun$9.apply(TensorflowAlbert.scala:156)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:156)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6$$anonfun$7.apply(TensorflowAlbert.scala:154)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:154)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1$$anonfun$apply$6.apply(TensorflowAlbert.scala:152)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:152)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert$$anonfun$calculateEmbeddings$1.apply(TensorflowAlbert.scala:138)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowAlbert.calculateEmbeddings(TensorflowAlbert.scala:138)\n\tat com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings.annotate(AlbertEmbeddings.scala:139)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:35)\n\tat com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:34)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "(df_bert\n",
    " .select('entry_id', 'version', 'sentences', 'embeddings_vectors')\n",
    " .write.mode(\"overwrite\").parquet(\"s3://aspangher/tmp/tmp_albert_embeddings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert.select('entry_id', 'version', 'embeddings_vectors').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = bert_pipeline_from_sentences.fit(spark.createDataFrame([[\"\"]]).toDF(\"summary\"))\n",
    "result = pipeline_model.transform(spark.createDataFrame(pd.DataFrame({\"summary\": [\"I love NLP\"]})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_df = (df_bert\n",
    "         .select('entry_id', 'version', 'sent_idx', 'sentence', 'embeddings_vectors')\n",
    "         .toPandas()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = CoNLL().readDataset(spark, 's3://aspangher/spark-nlp/conll/eng.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "training_data = CoNLL().readDataset(spark, 's3://aspangher/spark-nlp/conll/eng.train')\n",
    "\n",
    "get_embeddings = (sa.AlbertEmbeddings\n",
    "        .load('s3://aspangher/spark-nlp/albert_large_uncased_en')\n",
    "        .setInputCols(\"sentence\", \"token\")\n",
    "        .setOutputCol(\"embeddings\")\n",
    "        .setMaxSentenceLength(100)\n",
    "        .setBatchSize(8)\n",
    ")\n",
    "\n",
    "embeddings_finisher = (\n",
    "    sb.EmbeddingsFinisher()\n",
    "        .setInputCols(\"embeddings\")\n",
    "        .setOutputCols(\"embeddings_vectors\")\n",
    "        .setOutputAsVector(True)\n",
    ")\n",
    "\n",
    "sentence_finisher = (\n",
    "    Finisher()\n",
    "       .setInputCols([\"sentence\"]) \n",
    ")\n",
    "\n",
    "pipeline =  Pipeline(stages=[\n",
    "    get_embeddings, \n",
    "    embeddings_finisher, \n",
    "    sentence_finisher\n",
    "])\n",
    "\n",
    "pipelineDF = pipeline.fit(training_data).transform(training_data)\n",
    "\n",
    "(pipelineDF\n",
    " .select('finished_sentence', 'embeddings_vectors')\n",
    " .write\n",
    " .mode(\"overwrite\").parquet('s3://aspangher/tmp/tmp_conll_albert_embeddings.pq')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.5'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineDF.select('finished_sentence', 'embeddings_vectors').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = RecursivePipeline(stages=[\n",
    "    DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\"), \n",
    "    SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\"), \n",
    "    Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\").setMaxLength(100).setSplitChars([\"-\", \"\\xa0\", \"—\"]), \n",
    "    BertEmbeddings.pretrained(name = \"bert_large_cased\", lang='en').setInputCols(['sentence', 'token']).setOutputCol('embeddings'), \n",
    "#     NerDLModel.pretrained('onto_bert_large_cased', 'en').setInputCols(['sentence', 'token', 'embeddings']).setOutputCol('ner'), \n",
    "#     NerConverter().setInputCols(['sentence', 'token', 'ner']).setOutputCol('ner_chunk') \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

contextual_layer_type: lstm
dataset_size: 47500
do_regression: true
embedding_model_type: roberta
experiment: sentence
gradient_accumulation: 1
learning_rate: 0.0001
lstm_bidirectional: false
lstm_num_hidden_layers: 2
notes: Addition, Poisson Regression, Sentence Level, Docs < 15,> 5, downsampled 50,000
num_sent_attn_heads: 2
num_warmup_steps: 0
use_doc_embs: true
use_poisson_regression: true
use_pos_embs: true
